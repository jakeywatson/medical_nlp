{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2nzdtSRYixE8"
   },
   "source": [
    "# Machine-Learning Drug-Name Classifier\n",
    "This notebook contains the feature extractor for the machine-learning classifier for the AHLT course, UPC.\n",
    "\n",
    "Author: Jake Watson, 22/03/2020\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R9MsohSnK7Cq"
   },
   "outputs": [],
   "source": [
    "import xml.dom\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.tokenize import string_span_tokenize\n",
    "from xml.dom.minidom import parse\n",
    "from nltk.metrics import *\n",
    "from nltk.tokenize.util import align_tokens\n",
    "from collections import OrderedDict\n",
    "from google.colab import drive\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "\n",
    "\n",
    "def install_java():\n",
    "  !apt-get install -y openjdk-8-jdk-headless -qq > /dev/null      #install openjdk\n",
    "  os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"     #set environment variable\n",
    "  !java -version       #check java version\n",
    "install_java()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OqcUhnDJVGbB"
   },
   "outputs": [],
   "source": [
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S8AQc_MVi7C4"
   },
   "source": [
    "Loads the data taken from the training set.\n",
    "```\n",
    "read_saved()\n",
    "```\n",
    "Loads the data taken from the training set and external knowledge sources\n",
    "```\n",
    "read_saved_large()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VCCmxxLyxwaO"
   },
   "outputs": [],
   "source": [
    "entities, prefixes, suffixes, non_entities = read_saved()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dwy8pzWQx1ZV"
   },
   "outputs": [],
   "source": [
    "entities, prefixes, suffixes, non_entities = read_saved_large()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DvVn4Sbvi9p0"
   },
   "source": [
    "Saves the features taken from the training set.\n",
    "```\n",
    "save_features()\n",
    "```\n",
    "Saves the features taken from the training set and external knowledge sources\n",
    "```\n",
    "read_saved_large()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TkRltvbMahlV"
   },
   "outputs": [],
   "source": [
    "save_features('/content/drive/My Drive/UPC/Semester2/data/Train/', entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M5a591FayW1i"
   },
   "outputs": [],
   "source": [
    "save_features_large(entities, non_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4joyAYqSAtir"
   },
   "outputs": [],
   "source": [
    "# Tokenizes the input text. \n",
    "# Returns the tokens, with their offsets from the beginning of the sentence.\n",
    "\n",
    "def tokenize(input):\n",
    "  s = input.replace('\"', \"'\")\n",
    "  tokens = TreebankWordTokenizer().tokenize(s)\n",
    "  offsets = list(align_tokens(tokens, s))\n",
    "  offsets = [tuple((i, j-1)) for i, j in offsets]\n",
    "  output = [tuple((i, j)) for i, j in zip(tokens, offsets)]\n",
    "  return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mLU9h1FkjarO"
   },
   "source": [
    "# Features\n",
    "To feed into the learner, we need to extract a set of features for each token in a sentence. These are similar to the rules in the Rule-Based classifier:\n",
    "*   Capitalised: is the token entirely capitalised?\n",
    "*   Has Capitals: does the token contain any capitals?\n",
    "*   Part of Speech: is the token at the beginning or end of the sentence?\n",
    "*   Dashes: does the token contain dashes (-) ?\n",
    "*   Numbers: does the token contain numbers?\n",
    "*   Suffixes/Prefixes: extracts the suffixes and prefixes of lengths 1-5 for each token\n",
    "*   Known: is the token in the list of known entities? If so, what is its BIO tag? (as evaluated using the lists of known entities, not from the training set XML). Uses the sliding-window technique to find entity names of up to 5 tokens.\n",
    "\n",
    "As well as the features, the sentence is also labelled with a set of B-I-O tags, indicating that a token either Begins, is In, or is Outside of an entity name. These tags are taken from the annotated training set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A0OTqj4LT1q3"
   },
   "outputs": [],
   "source": [
    "# Given an input set of tokens, extracts a set of binary features.\n",
    "\n",
    "def extract_features(sentence, entities):\n",
    "  tokens = [sentence[i][0] for i in range(len(sentence))]\n",
    "  features = []\n",
    "\n",
    "  labels = sliding_window(sentence, entities)\n",
    "\n",
    "  for i in range(len(tokens)):\n",
    "    token = tokens[i]\n",
    "\n",
    "    pre1 = token[:1]\n",
    "    pre2 = token[:2]\n",
    "    pre3 = token[:3]\n",
    "    pre4 = token[:4]\n",
    "    pre5 = token[:5]\n",
    "\n",
    "    suf1 = token[-1:]\n",
    "    suf2 = token[-2:]\n",
    "    suf3 = token[-3:]\n",
    "    suf4 = token[-4:]\n",
    "    suf5 = token[-5:]\n",
    "\n",
    "    caps = is_capitalised(token)\n",
    "    contains_caps = has_capitals(token)\n",
    "    numbers = has_numbers(token)\n",
    "    dashes = has_dashes(token)\n",
    "\n",
    "    dict_type = labels[i]\n",
    "\n",
    "    prev = \"BoS\"\n",
    "    nxt = \"EoS\"\n",
    "    if (i > 0):\n",
    "      prev = tokens[i-1]\n",
    "    if (i < len(tokens)-1):\n",
    "      nxt = tokens[i+1]\n",
    "    \n",
    "    vector = OrderedDict()\n",
    "    vector[\"form\"]=token\n",
    "    vector[\"pre1\"]=pre1 \n",
    "    vector[\"pre2\"]=pre2 \n",
    "    vector[\"pre3\"]=pre3 \n",
    "    vector[\"pre4\"]=pre4 \n",
    "    vector[\"pre5\"]=pre5 \n",
    "    vector[\"suf1\"]=suf1 \n",
    "    vector[\"suf2\"]=suf2 \n",
    "    vector[\"suf3\"]=suf3 \n",
    "    vector[\"suf4\"]=suf4 \n",
    "    vector[\"suf5\"]=suf5 \n",
    "    vector[\"caps\"]=str(caps)\n",
    "    vector[\"has_caps\"]=str(contains_caps)\n",
    "    vector[\"has_nums\"]=str(numbers)\n",
    "    vector[\"has_dash\"]=str(dashes)\n",
    "    vector[\"known_type\"]=str(dict_type)\n",
    "    vector[\"prev\"]=prev\n",
    "    vector[\"next\"]=nxt\n",
    "\n",
    "    features.append(vector)\n",
    "  return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OymkLPiJxHLP"
   },
   "outputs": [],
   "source": [
    "# Uses the known offsets of the entities in a sentence to label the tokens with their BIO tags\n",
    "\n",
    "def extract_bio_tag(span_start, span_end, sentence_entities):\n",
    "  for entity in sentence_entities:\n",
    "    for e_span in entity[1]:\n",
    "      if (span_start == e_span[0]):\n",
    "        return \"B-\"+entity[2]\n",
    "      elif (span_start > e_span[0] and span_end <= e_span[1]):\n",
    "        return \"I-\"+entity[2]\n",
    "  return \"O\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4PYbnwXVt7u1"
   },
   "outputs": [],
   "source": [
    "# Outputs the features for a set of tokens in the format requieed by the evaluator\n",
    "\n",
    "def output_features(id, tokens, entities, features, outfile):\n",
    "  for i in range(len(tokens)):\n",
    "    name = tokens[i][0]\n",
    "    offsets = tokens[i][1]\n",
    "\n",
    "    feature = features[i]\n",
    "\n",
    "    span_start = offsets[0]\n",
    "    span_end = offsets[1]\n",
    "\n",
    "    gold_class = extract_bio_tag(span_start, span_end, entities)\n",
    "\n",
    "    feats = \"\"\n",
    "    for f in feature.keys():\n",
    "      feats += \"\\t\" + f + \"=\" + feature[f]\n",
    "    \n",
    "    outstring = str(id) + \"\\t\" + str(name) + \"\\t\" + str(span_start) + \"\\t\" + str(span_end) + \"\\t\" + gold_class + feats + \"\\n\"\n",
    "    outfile.write(outstring)\n",
    "  outfile.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ruNlt1QYhocb"
   },
   "outputs": [],
   "source": [
    "def is_capitalised(token):\n",
    "  if token.isupper():\n",
    "    return 1\n",
    "  return 0\n",
    "\n",
    "def has_capitals(token):\n",
    "  if (token.isupper() == False) and any(x.isupper() for x in token):\n",
    "    return 1\n",
    "  else: \n",
    "    return 0\n",
    "\n",
    "def has_dashes(token):\n",
    "  if (\"-\" in token):\n",
    "    return 1\n",
    "  return 0\n",
    "\n",
    "def has_numbers(token):\n",
    "  if any(i.isdigit() for i in token):\n",
    "    return 1\n",
    "  return 0\n",
    "\n",
    "def ends_with_s(token):\n",
    "  if token[-1:].casefold().strip() == 's':\n",
    "    return 1\n",
    "  else: \n",
    "    return 0\n",
    "\n",
    "def has_suffix(token, suffs):\n",
    "  for suffix in suffs:\n",
    "    if (suffix == token.casefold()[-len(suffix.strip()):]):\n",
    "      return 1\n",
    "  return 0\n",
    "\n",
    "def has_prefix(token, prefs):\n",
    "  for prefix in prefs:\n",
    "    if (prefix == token.casefold()[:len(prefix.strip())]):\n",
    "      return 1\n",
    "  return 0\n",
    "\n",
    "def in_dictionary(token, dictionary):\n",
    "  punctuations = '''![]{};'\"\\,<>./?@#$%^&*~'''\n",
    "\n",
    "  no_punct = token\n",
    "  for char in punctuations:\n",
    "    no_punct = no_punct.replace(char, \" \")\n",
    "  token = no_punct.casefold().strip()\n",
    "\n",
    "  if token in dictionary:\n",
    "    return True\n",
    "  return False\n",
    "\n",
    "\n",
    "def in_entities(token, entities):\n",
    "  punctuations = '''![]{};,'\"\\<>./?@#$%^&*~'''\n",
    "  to_find = token.casefold().strip()\n",
    "\n",
    "  no_punct = to_find\n",
    "  for char in punctuations:\n",
    "    no_punct = no_punct.replace(char, \" \")\n",
    "  to_find = no_punct.casefold().strip()\n",
    "\n",
    "  for label in entities.keys():\n",
    "    if in_dictionary(to_find, entities[label]):\n",
    "      return True, label\n",
    "\n",
    "  return False, \"unknown\"\n",
    "\n",
    "def sliding_window(tokens, entities):\n",
    "  punctuations = '''![]{};,'\"\\<>./?@#$%^&*~'''\n",
    "  classes = []\n",
    "  for token in tokens:\n",
    "    classes.append(\"O\")\n",
    "\n",
    "  for i in range(0, len(tokens)-1):\n",
    "    if (classes[i] is not \"O\"):\n",
    "      next\n",
    "    else:\n",
    "      tmp = tokens[i][0]\n",
    "      if any(x in tmp for x in punctuations):\n",
    "        next\n",
    "\n",
    "      known, label = in_entities(tmp.casefold(), entities)\n",
    "      if known:\n",
    "        classes[i] = \"B-\"+label\n",
    "        tmp = \"\"\n",
    "        next\n",
    "\n",
    "      for j in range(1, 5):\n",
    "        if i+j == len(tokens):\n",
    "          break\n",
    "        if classes[i+j] != \"O\":\n",
    "          break\n",
    "        tmp = tmp + \" \" + tokens[i+j][0]\n",
    "        if any(x in tmp for x in punctuations):\n",
    "          break\n",
    "        known, label = in_entities(tmp.casefold(), entities)\n",
    "        if known:\n",
    "          for x in range(i, i + j + 1):\n",
    "            if x == i:\n",
    "              classes[x] = \"B-\"+label\n",
    "            else:\n",
    "              classes[x] = \"I-\"+label\n",
    "          break\n",
    "\n",
    "  return classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VfgyKTIZlJ_y"
   },
   "source": [
    "# Reading and Writing\n",
    "Functions to write the features to file, for use by the learner.\n",
    "\n",
    "1.   **features.txt: ** Contains features extracted from the training set.\n",
    "2.   **features_large.txt: ** Contains features extracted from the training set and external sources.\n",
    "3.   **trained.crfsuite: ** Contains trained model extracted from the training set.\n",
    "4.   **trained_large.crfsuite: ** Contains trained model extracted from the training set and external sources.\n",
    "\n",
    "**External data**\n",
    "\n",
    "The external data is in a different form to the training set: it consists of annotated entities with no extraneous words. This leads to biasing of the Position of Speech feature, as entities in the external data are always the only words in the sentence. To combat this, I extracted a random set of non-entity words of random length, and padded each side of the entities. \n",
    "\n",
    "**Sources**\n",
    "\n",
    "The sources of the external data were the DrugBank annotated file, HSDB annotated file, and the 'EN' set of random English sentences, found in the lab directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kgQT3y-qVmZI"
   },
   "outputs": [],
   "source": [
    "def save_features(inputdir, known_entities):\n",
    "  outfile = open('/content/drive/My Drive/UPC/Semester2/features/features.txt', 'w+')\n",
    "  for f in os.listdir(inputdir):\n",
    "    parsed = parse(open(str(inputdir)+str(f)))\n",
    "    sentences = parsed.getElementsByTagName(\"sentence\")\n",
    "    for sentence in sentences:\n",
    "      sentence_id = sentence.getAttribute('id')\n",
    "      \n",
    "      entities = []\n",
    "      for child in sentence.childNodes:\n",
    "        if child.nodeType == xml.dom.minidom.Node.ELEMENT_NODE:\n",
    "          if child.tagName == 'entity':\n",
    "            name = child.getAttribute('text')\n",
    "            offsets = []\n",
    "            multi = child.getAttribute('charOffset').split(';')\n",
    "            for offset in multi:\n",
    "              offsets.append(list(map(int, offset.split('-'))))\n",
    "            label = child.getAttribute('type')\n",
    "            entities.append([name, offsets, label])\n",
    "\n",
    "      tokens = tokenize(sentence.attributes[\"text\"].value)\n",
    "      features = extract_features(tokens, known_entities)\n",
    "      output_features(sentence_id, tokens, entities, features, outfile)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2OrXz58zpnHa"
   },
   "outputs": [],
   "source": [
    "def save_features_large(known_entities, non_entities):\n",
    "  outfile = open('/content/drive/My Drive/UPC/Semester2/features/features_large.txt', 'w+')\n",
    "\n",
    "  inputdir = '/content/drive/My Drive/UPC/Semester2/data/Train/'\n",
    "  for f in os.listdir(inputdir):\n",
    "    parsed = parse(open(str(inputdir)+str(f)))\n",
    "    sentences = parsed.getElementsByTagName(\"sentence\")\n",
    "    for sentence in sentences:\n",
    "      sentence_id = sentence.getAttribute('id')\n",
    "      \n",
    "      entities = []\n",
    "      for child in sentence.childNodes:\n",
    "        if child.nodeType == xml.dom.minidom.Node.ELEMENT_NODE:\n",
    "          if child.tagName == 'entity':\n",
    "            name = child.getAttribute('text')\n",
    "            offsets = []\n",
    "            multi = child.getAttribute('charOffset').split(';')\n",
    "            for offset in multi:\n",
    "              offsets.append(list(map(int, offset.split('-'))))\n",
    "            label = child.getAttribute('type')\n",
    "            entities.append([name, offsets, label])\n",
    "\n",
    "      tokens = tokenize(sentence.attributes[\"text\"].value)\n",
    "      features = extract_features(tokens, known_entities)\n",
    "      output_features(sentence_id, tokens, entities, features, outfile)\n",
    "    \n",
    "  drugbank = open('drive/My Drive/UPC/Semester2/external/DrugBank.txt')#\n",
    "  hsdb = open('drive/My Drive/UPC/Semester2/external/HSDB.txt')\n",
    "  en = open('drive/My Drive/UPC/Semester2/external/en.txt')\n",
    "\n",
    "  counter = 0\n",
    "  for line in drugbank.read().splitlines():\n",
    "    counter += 1\n",
    "    split = line.split(\"|\")\n",
    "    entity = split[0]\n",
    "    label = split[1]\n",
    "    padded_entity, n_start_words, n_end_words = insert_random_padding(entity, non_entities)\n",
    "    tokens = tokenize(padded_entity)\n",
    "    features = extract_features(tokens, known_entities)\n",
    "    id = \"drugbank_sentence_\" + str(counter)\n",
    "    outstrings = extract_bio_tag_from_padded_entity(id, tokens, features, padded_entity, n_start_words, n_end_words, label)\n",
    "    for string in outstrings:  \n",
    "      outfile.write(string) \n",
    "    outfile.write(\"\\n\")\n",
    "    \n",
    "  counter = 0\n",
    "  for line in hsdb.read().splitlines():\n",
    "    counter += 1\n",
    "    entity = line.casefold().strip()\n",
    "    padded_entity, n_start_words, n_end_words = insert_random_padding(entity, non_entities)\n",
    "    tokens = tokenize(padded_entity)\n",
    "    features=extract_features(tokens, known_entities)\n",
    "    id = \"hsdb_sentence_\" + str(counter)\n",
    "    outstrings = extract_bio_tag_from_padded_entity(id, tokens, features, padded_entity, n_start_words, n_end_words, \"drug_n\")\n",
    "    for string in outstrings:\n",
    "      outfile.write(string)\n",
    "    outfile.write(\"\\n\")\n",
    "  hsdb.close()\n",
    "\n",
    "  counter = 0\n",
    "  for line in en.read().splitlines():\n",
    "    counter += 1\n",
    "    id = \"en_sentence_\" + str(counter)\n",
    "    tokens = tokenize(line)\n",
    "    features = extract_features(tokens, known_entities)\n",
    "    for i in range(len(tokens)):\n",
    "      name = tokens[i][0]\n",
    "      offsets = tokens[i][1]\n",
    "      feature = features[i]\n",
    "      span_start = offsets[0]\n",
    "      span_end = offsets[1]\n",
    "      gold_class = \"O\"\n",
    "      feats = \"\"\n",
    "      for f in feature.keys():\n",
    "        feats += \"\\t\" + f + \"=\" + feature[f]\n",
    "      outstring = str(id) + \"\\t\" + str(name) + \"\\t\" + str(span_start) + \"\\t\" + str(span_end) + \"\\t\" + gold_class + feats + \"\\n\"\n",
    "      outfile.write(outstring)\n",
    "    outfile.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-ta6os29t8_s"
   },
   "outputs": [],
   "source": [
    "# Used exclusively to process the external data\n",
    "\n",
    "def insert_random_padding(string, non_entities):\n",
    "  n_start_words = random.randint(0, 4)\n",
    "  n_end_words = random.randint(0, 4)\n",
    "\n",
    "  start_words = random.sample(non_entities, n_start_words)\n",
    "  end_words = random.sample(non_entities, n_end_words)\n",
    "\n",
    "  start_string = \" \".join([str(elem) for elem in start_words]) + \" \"\n",
    "  end_string = \" \" + \" \".join([str(elem) for elem in end_words]) + \".\"\n",
    "\n",
    "  start_offset = len(start_string)\n",
    "  end_offset = len(end_string)\n",
    "\n",
    "  padded_entity = str(start_string) + str(string) + str(end_string)\n",
    "  return padded_entity, n_start_words, n_end_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q8t5iry2uMre"
   },
   "outputs": [],
   "source": [
    "def extract_bio_tag_from_padded_entity(id, tokens, features, entity, n_start_words, n_end_words, label):\n",
    "  outstrings = []\n",
    "  for i in range(len(tokens)):\n",
    "    name = tokens[i][0]\n",
    "    offsets = tokens[i][1]\n",
    "    feature = features[i]\n",
    "\n",
    "    span_start = offsets[0]\n",
    "    span_end = offsets[1]\n",
    "\n",
    "    gold_class = \"O\"\n",
    "    if i == n_start_words:\n",
    "      gold_class = \"B-\" + label\n",
    "    elif i > n_start_words and i < len(tokens) - n_end_words - 1:\n",
    "      gold_class = \"I-\" + label\n",
    "\n",
    "    feats = \"\"\n",
    "    for f in feature.keys():\n",
    "      feats += \"\\t\" + f + \"=\" + feature[f]\n",
    "    \n",
    "    outstring = str(id) + \"\\t\" + str(name) + \"\\t\" + str(span_start) + \"\\t\" + str(span_end) + \"\\t\" + gold_class + feats + \"\\n\"\n",
    "    outstrings.append(outstring)\n",
    "\n",
    "  return outstrings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mwx2QjsbxZtm"
   },
   "outputs": [],
   "source": [
    "def read_saved():\n",
    "  prefixes = dict()\n",
    "  prefixes[\"drug\"] = set()\n",
    "  prefixes[\"drug_n\"] = set()\n",
    "  prefixes[\"brand\"] = set()\n",
    "  prefixes[\"group\"] = set()\n",
    "  \n",
    "  suffixes = dict()\n",
    "  suffixes[\"drug\"] = set()\n",
    "  suffixes[\"drug_n\"] = set()\n",
    "  suffixes[\"group\"] = set()\n",
    "  suffixes[\"brand\"] = set()  \n",
    "\n",
    "  entities = dict()\n",
    "  entities[\"drug\"] = set()\n",
    "  entities[\"drug_n\"] = set()\n",
    "  entities[\"group\"] = set()\n",
    "  entities[\"brand\"] = set()\n",
    "\n",
    "  non_entities = set()\n",
    "\n",
    "  prefix_file = open(\"/content/drive/My Drive/UPC/Semester2/data/extracted/prefixes.txt\")\n",
    "  for line in prefix_file.read().splitlines():\n",
    "    split = line.split(\":\")\n",
    "    e_type = split[0]\n",
    "    name = split[1]\n",
    "    prefixes[e_type].add(name.casefold().strip())\n",
    "  prefix_file.close()\n",
    "\n",
    "  suffix_file = open(\"/content/drive/My Drive/UPC/Semester2/data/extracted/suffixes.txt\")\n",
    "  for line in suffix_file.read().splitlines():\n",
    "    split = line.split(\":\")\n",
    "    e_type = split[0]\n",
    "    name = split[1]\n",
    "    suffixes[e_type].add(name.casefold().strip())\n",
    "  suffix_file.close()\n",
    "\n",
    "  entities_file = open(\"/content/drive/My Drive/UPC/Semester2/data/extracted/entities.txt\")\n",
    "  for line in entities_file.read().splitlines():\n",
    "    split = line.split(\":\")\n",
    "    e_type = split[0]\n",
    "    name = split[1]\n",
    "    entities[e_type].add(name.casefold().strip())\n",
    "  entities_file.close()\n",
    "\n",
    "  non_entities_file = open(\"/content/drive/My Drive/UPC/Semester2/data/extracted/non_entities.txt\")\n",
    "  for line in non_entities_file.read().splitlines():\n",
    "    non_entities.add(line.casefold().strip())\n",
    "  non_entities_file.close()\n",
    "\n",
    "  return entities, prefixes, suffixes, non_entities;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5sJvj5Vpw5XE"
   },
   "outputs": [],
   "source": [
    "def read_saved_large():\n",
    "  prefixes = dict()\n",
    "  prefixes[\"drug\"] = set()\n",
    "  prefixes[\"drug_n\"] = set()\n",
    "  prefixes[\"group\"] = set()\n",
    "  prefixes[\"brand\"] = set()  \n",
    "\n",
    "  suffixes = dict()\n",
    "  suffixes[\"drug\"] = set()\n",
    "  suffixes[\"drug_n\"] = set()\n",
    "  suffixes[\"group\"] = set()\n",
    "  suffixes[\"brand\"] = set()  \n",
    "\n",
    "  entities = dict()\n",
    "  entities[\"drug\"] = set()\n",
    "  entities[\"drug_n\"] = set()\n",
    "  entities[\"group\"] = set()\n",
    "  entities[\"brand\"] = set()\n",
    "\n",
    "  non_entities = set()\n",
    "\n",
    "  prefix_file = open(\"/content/drive/My Drive/UPC/Semester2/data/extracted/prefixes_large.txt\")\n",
    "  for line in prefix_file.read().splitlines():\n",
    "    split = line.split(\":\")\n",
    "    e_type = split[0]\n",
    "    name = split[1]\n",
    "    prefixes[e_type].add(name.casefold().strip())\n",
    "  prefix_file.close()\n",
    "\n",
    "  suffix_file = open(\"/content/drive/My Drive/UPC/Semester2/data/extracted/suffixes_large.txt\")\n",
    "  for line in suffix_file.read().splitlines():\n",
    "    split = line.split(\":\")\n",
    "    e_type = split[0]\n",
    "    name = split[1]\n",
    "    suffixes[e_type].add(name.casefold().strip())\n",
    "  suffix_file.close()\n",
    "\n",
    "  entities_file = open(\"/content/drive/My Drive/UPC/Semester2/data/extracted/entities_large.txt\")\n",
    "  for line in entities_file.read().splitlines():\n",
    "    split = line.split(\":\")\n",
    "    e_type = split[0]\n",
    "    name = split[1]\n",
    "    entities[e_type].add(name.casefold().strip())\n",
    "  entities_file.close()\n",
    "\n",
    "  non_entities_file = open(\"/content/drive/My Drive/UPC/Semester2/data/extracted/non_entities_large.txt\")\n",
    "  for line in non_entities_file.read().splitlines():\n",
    "    non_entities.add(line.casefold().strip())\n",
    "  non_entities_file.close()\n",
    "\n",
    "  return entities, prefixes, suffixes, non_entities;"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "AHLT_LAB_1_FEATURE_EXTRACTOR.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
