{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tpg8VTcAoJcj"
   },
   "source": [
    "# Machine-Learning Drug-Name Classifier\n",
    "This notebook contains the machine-learning classifier for the AHLT course, UPC.\n",
    "\n",
    "Author: Jake Watson, 22/03/2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R9MsohSnK7Cq"
   },
   "outputs": [],
   "source": [
    "import xml.dom\n",
    "from xml.dom.minidom import parse\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.tokenize import string_span_tokenize\n",
    "from nltk.tokenize.util import align_tokens\n",
    "from google.colab import drive\n",
    "\n",
    "import copy\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "id": "CmYRCtmsBuVU",
    "outputId": "a9d03448-8b65-49c8-f0ca-95f1c66f9daa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"11.0.6\" 2020-01-14\n",
      "OpenJDK Runtime Environment (build 11.0.6+10-post-Ubuntu-1ubuntu118.04.1)\n",
      "OpenJDK 64-Bit Server VM (build 11.0.6+10-post-Ubuntu-1ubuntu118.04.1, mixed mode, sharing)\n"
     ]
    }
   ],
   "source": [
    "def install_pycrf():\n",
    "  !pip install -q python-crfsuite\n",
    "install_pycrf()\n",
    "def install_java():\n",
    "  !apt-get install -y openjdk-8-jdk-headless -qq > /dev/null      #install openjdk\n",
    "  os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"     #set environment variable\n",
    "  !java -version       #check java version\n",
    "install_java()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lVYkH7tTFFit"
   },
   "outputs": [],
   "source": [
    "import pycrfsuite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "id": "5xO4MuRABweR",
    "outputId": "a2f4bcf6-8c7d-4a5f-c828-dc4f920bb853"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vi8lPrEpoSn-"
   },
   "source": [
    "Both cells below do the same job, but have different data sources. Choose the top one to read only internal knowledge, and train the classifier on that. Choose the lower one to read both internal and external knowledge, and train the classifier on that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DrmMdr5zirEe"
   },
   "outputs": [],
   "source": [
    "entities, prefixes, suffixes, non_entities = read_saved()\n",
    "tags, features = readFeatures('drive/My Drive/UPC/Semester2/features/features.txt')\n",
    "train('drive/My Drive/UPC/Semester2/features/trained.crfsuite', features, tags, 1, 0.1, 50, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D5Ak7LDvE-cM"
   },
   "outputs": [],
   "source": [
    "entities, prefixes, suffixes, non_entities = read_saved_large()\n",
    "tags, features = readFeatures('drive/My Drive/UPC/Semester2/features/features_large.txt')\n",
    "train('drive/My Drive/UPC/Semester2/features/trained_large.crfsuite', features, tags, 0.1, 0.001, 50, True, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qswgYQ2WpIBM"
   },
   "source": [
    "The following two fields show instances of the evaluator output for the NERC function, for the 'Devel' and 'Test-NER' datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "iiKShRkC6oBE",
    "outputId": "82cdd9e2-9315-4b6e-a767-31783cd35693"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Considering file 112/112: Terconazole.xmlGold drive/My Drive/UPC/Semester2/data/Test-NER/\n",
      "Submission  task9.1_MLinternalTest_1.txt\n",
      "Directory gold drive/My Drive/UPC/Semester2/data/Test-NER/\n",
      "[transDirXMLToMapEntities] dir:drive/My Drive/UPC/Semester2/data/Test-NER/\n",
      "log4j:WARN No appenders could be found for logger (org.castor.core.util.Configuration).\n",
      "log4j:WARN Please initialize the log4j system properly.\n",
      " Gold standard saved in goldNER.txt\n",
      "\n",
      "Gold loaded. Sentences=324, entities: 686\n",
      "task9.1_MLinternalTest_1_scores.log created...\n",
      "SCORES FOR THE GROUP: MLinternalTest RUN=1\n",
      "\n",
      "Warning!!!! some sentences are no included in the gold!!!\n",
      "\n",
      "Strict matching (boundaries + type)\n",
      "cor\tinc\tpar\tmis\tspu\ttotal\tprec\trecall\tF1\n",
      "306\t26\t0\t354\t22\t686\t0.86\t0.45\t0.59\n",
      "\n",
      "\n",
      "\n",
      "Warning!!!! some sentences are no included in the gold!!!\n",
      "\n",
      "Exact matching\n",
      "cor\tinc\tpar\tmis\tspu\ttotal\tprec\trecall\tF1\n",
      "313\t19\t0\t354\t22\t686\t0.88\t0.46\t0.6\n",
      "\n",
      "\n",
      "\n",
      "Warning!!!! some sentences are no included in the gold!!!\n",
      "\n",
      "Partial matching\n",
      "cor\tinc\tpar\tmis\tspu\ttotal\tprec\trecall\tF1\n",
      "313\t0\t19\t354\t22\t686\t0.88\t0.47\t0.61\n",
      "\n",
      "\n",
      "\n",
      "Warning!!!! some sentences are no included in the gold!!!\n",
      "\n",
      "type matching\n",
      "cor\tinc\tpar\tmis\tspu\ttotal\tprec\trecall\tF1\n",
      "320\t12\t0\t354\t22\t686\t0.9\t0.47\t0.62\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "SCORES FOR ENTITY TYPE\n",
      "Warning!!!! some sentences are no included in the gold!!!\n",
      "\n",
      "Exact matching on drug\n",
      "cor\tinc\tpar\tmis\tspu\ttotal\tprec\trecall\tF1\n",
      "193\t4\t0\t154\t10\t351\t0.93\t0.55\t0.69\n",
      "\n",
      "\n",
      "Exact matching on brand\n",
      "cor\tinc\tpar\tmis\tspu\ttotal\tprec\trecall\tF1\n",
      "10\t0\t0\t49\t0\t59\t1\t0.17\t0.29\n",
      "\n",
      "\n",
      "Warning!!!! some sentences are no included in the gold!!!\n",
      "\n",
      "Exact matching on group\n",
      "cor\tinc\tpar\tmis\tspu\ttotal\tprec\trecall\tF1\n",
      "98\t10\t0\t47\t6\t155\t0.86\t0.63\t0.73\n",
      "\n",
      "\n",
      "Exact matching on drug_n\n",
      "cor\tinc\tpar\tmis\tspu\ttotal\tprec\trecall\tF1\n",
      "5\t0\t0\t116\t1\t121\t0.83\t0.04\t0.08\n",
      "\n",
      "\n",
      "MACRO-AVERAGE MEASURES:\n",
      "P\tR\tF1\n",
      "0.91\t0.35\t0.45\n",
      "________________________________________________________________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nerc('drive/My Drive/UPC/Semester2/data/Test-NER/','task9.1_MLinternalTest_1.txt', entities, 'drive/My Drive/UPC/Semester2/features/trained.crfsuite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "CTfxnz_yidMW",
    "outputId": "a6786e3c-8a11-4225-cfb1-344fff8fc61e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Considering file 112/112: Terconazole.xmlGold drive/My Drive/UPC/Semester2/data/Test-NER/\n",
      "Submission  task9.1_MLexternalTest_1.txt\n",
      "Directory gold drive/My Drive/UPC/Semester2/data/Test-NER/\n",
      "[transDirXMLToMapEntities] dir:drive/My Drive/UPC/Semester2/data/Test-NER/\n",
      "log4j:WARN No appenders could be found for logger (org.castor.core.util.Configuration).\n",
      "log4j:WARN Please initialize the log4j system properly.\n",
      " Gold standard saved in goldNER.txt\n",
      "\n",
      "Gold loaded. Sentences=324, entities: 686\n",
      "task9.1_MLexternalTest_1_scores.log created...\n",
      "SCORES FOR THE GROUP: MLexternalTest RUN=1\n",
      "\n",
      "Warning!!!! some sentences are no included in the gold!!!\n",
      "\n",
      "Strict matching (boundaries + type)\n",
      "cor\tinc\tpar\tmis\tspu\ttotal\tprec\trecall\tF1\n",
      "362\t80\t0\t244\t44\t686\t0.74\t0.53\t0.62\n",
      "\n",
      "\n",
      "\n",
      "Warning!!!! some sentences are no included in the gold!!!\n",
      "\n",
      "Exact matching\n",
      "cor\tinc\tpar\tmis\tspu\ttotal\tprec\trecall\tF1\n",
      "395\t47\t0\t244\t44\t686\t0.81\t0.58\t0.67\n",
      "\n",
      "\n",
      "\n",
      "Warning!!!! some sentences are no included in the gold!!!\n",
      "\n",
      "Partial matching\n",
      "cor\tinc\tpar\tmis\tspu\ttotal\tprec\trecall\tF1\n",
      "395\t0\t47\t244\t44\t686\t0.81\t0.61\t0.7\n",
      "\n",
      "\n",
      "\n",
      "Warning!!!! some sentences are no included in the gold!!!\n",
      "\n",
      "type matching\n",
      "cor\tinc\tpar\tmis\tspu\ttotal\tprec\trecall\tF1\n",
      "393\t49\t0\t244\t44\t686\t0.81\t0.57\t0.67\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "SCORES FOR ENTITY TYPE\n",
      "Warning!!!! some sentences are no included in the gold!!!\n",
      "\n",
      "Exact matching on drug\n",
      "cor\tinc\tpar\tmis\tspu\ttotal\tprec\trecall\tF1\n",
      "229\t4\t0\t118\t22\t351\t0.9\t0.65\t0.76\n",
      "\n",
      "\n",
      "Warning!!!! some sentences are no included in the gold!!!\n",
      "\n",
      "Exact matching on brand\n",
      "cor\tinc\tpar\tmis\tspu\ttotal\tprec\trecall\tF1\n",
      "29\t9\t0\t21\t0\t59\t0.76\t0.49\t0.6\n",
      "\n",
      "\n",
      "Warning!!!! some sentences are no included in the gold!!!\n",
      "\n",
      "Exact matching on group\n",
      "cor\tinc\tpar\tmis\tspu\ttotal\tprec\trecall\tF1\n",
      "97\t14\t0\t44\t5\t155\t0.84\t0.63\t0.72\n",
      "\n",
      "\n",
      "Warning!!!! some sentences are no included in the gold!!!\n",
      "\n",
      "Exact matching on drug_n\n",
      "cor\tinc\tpar\tmis\tspu\ttotal\tprec\trecall\tF1\n",
      "7\t5\t0\t109\t4\t121\t0.44\t0.06\t0.1\n",
      "\n",
      "\n",
      "MACRO-AVERAGE MEASURES:\n",
      "P\tR\tF1\n",
      "0.73\t0.46\t0.54\n",
      "________________________________________________________________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nerc('drive/My Drive/UPC/Semester2/data/Test-NER/','task9.1_MLexternalTest_1.txt', entities, 'drive/My Drive/UPC/Semester2/features/trained_large.crfsuite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9l2XC4hiPUJh"
   },
   "outputs": [],
   "source": [
    "# Main function: parses the XML files, extracts the sentences, tokenizes them, labels each token, outputs the results, and evaluate the results.\n",
    "\n",
    "def nerc(inputdir, outputfile, known_entities, trainfile):\n",
    "  output = open(outputfile, \"w+\")\n",
    "  count = 1\n",
    "  n_files = len(os.listdir(inputdir))\n",
    "  tagger = getTagger(trainfile)\n",
    "  for fil in os.listdir(inputdir):\n",
    "    sys.stdout.write(\"\\rConsidering file \" + str(count) + \"/\" + str(n_files) + \": \" + str(fil))\n",
    "    sys.stdout.flush()\n",
    "    count += 1\n",
    "    fil = open(str(inputdir) + str(fil))\n",
    "    tree = parse(fil)\n",
    "    fil.close()\n",
    "    sentences = tree.getElementsByTagName(\"sentence\")\n",
    "    for sentence in sentences:\n",
    "      sid = sentence.attributes[\"id\"].value\n",
    "      stext = sentence.attributes[\"text\"].value\n",
    "      tokens = tokenize(stext)\n",
    "      classes = classify(tokens, known_entities, tagger)\n",
    "      output_entities(sid, tokens, classes, output)\n",
    "  output.close\n",
    "  evaluate(inputdir, outputfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eWCEDJF3jtkC"
   },
   "outputs": [],
   "source": [
    "# Tokenizes the input text. \n",
    "# Returns the tokens, with their offsets from the beginning of the sentence.\n",
    "\n",
    "def tokenize(input):\n",
    "  s = input.replace('\"', \"'\")\n",
    "  tokens = TreebankWordTokenizer().tokenize(s)\n",
    "  offsets = list(align_tokens(tokens, s))\n",
    "  offsets = [tuple((i, j-1)) for i, j in offsets]\n",
    "  output = [tuple((i, j)) for i, j in zip(tokens, offsets)]\n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pewM2vh2GiCT"
   },
   "outputs": [],
   "source": [
    "# Given an input set of tokens, extracts a set of binary features.\n",
    "\n",
    "def extract_features(sentence, entities):\n",
    "  tokens = [sentence[i][0] for i in range(len(sentence))]\n",
    "  features = []\n",
    "  labels = sliding_window(sentence, entities)\n",
    "\n",
    "  for i in range(len(tokens)):\n",
    "    token = tokens[i]\n",
    "\n",
    "    pre1 = token[:1]\n",
    "    pre2 = token[:2]\n",
    "    pre3 = token[:3]\n",
    "    pre4 = token[:4]\n",
    "    pre5 = token[:5]\n",
    "\n",
    "    suf1 = token[-1:]\n",
    "    suf2 = token[-2:]\n",
    "    suf3 = token[-3:]\n",
    "    suf4 = token[-4:]\n",
    "    suf5 = token[-5:]\n",
    "\n",
    "    caps = is_capitalised(token)\n",
    "    contains_caps = has_capitals(token)\n",
    "    numbers = has_numbers(token)\n",
    "    dashes = has_dashes(token)\n",
    "\n",
    "    known_type = labels[i]    \n",
    "\n",
    "    prev = \"BoS\"\n",
    "    nxt = \"EoS\"\n",
    "    if (i > 0):\n",
    "      prev = tokens[i-1]\n",
    "    if (i < len(tokens)-1):\n",
    "      nxt = tokens[i+1]\n",
    "    \n",
    "    vector = []\n",
    "    vector.append(\"form=\"+token)\n",
    "    vector.append(\"pre=\"+pre1) \n",
    "    vector.append(\"pre2=\"+pre2) \n",
    "    vector.append(\"pre3=\"+pre3)\n",
    "    vector.append(\"pre4=\"+pre4)\n",
    "    vector.append(\"pre5=\"+pre5)\n",
    "    vector.append(\"suf1=\"+suf1)\n",
    "    vector.append(\"suf2=\"+suf2)\n",
    "    vector.append(\"suf3=\"+suf3)\n",
    "    vector.append(\"suf4=\"+suf4)\n",
    "    vector.append(\"suf5=\"+suf5)\n",
    "    vector.append(\"caps=\"+str(caps))\n",
    "    vector.append(\"has_caps=\"+str(contains_caps))\n",
    "    vector.append(\"has_nums=\"+str(numbers))\n",
    "    vector.append(\"has_dash=\"+str(dashes))\n",
    "    vector.append(\"known_type=\"+str(known_type))\n",
    "    vector.append(\"prev=\"+prev)\n",
    "    vector.append(\"next=\"+nxt)\n",
    "\n",
    "    features.append(vector)\n",
    "  return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "569xFpKcpa2a"
   },
   "outputs": [],
   "source": [
    "# Set of functions used to extract feature vectors.\n",
    "\n",
    "def is_capitalised(token):\n",
    "  if token.isupper():\n",
    "    return 1\n",
    "  return 0\n",
    "\n",
    "def has_capitals(token):\n",
    "  if (token.isupper() == False) and any(x.isupper() for x in token):\n",
    "    return 1\n",
    "  else: \n",
    "    return 0\n",
    "\n",
    "def has_dashes(token):\n",
    "  if (\"-\" in token):\n",
    "    return 1\n",
    "  return 0\n",
    "\n",
    "def has_numbers(token):\n",
    "  if any(i.isdigit() for i in token):\n",
    "    return 1\n",
    "  return 0\n",
    "\n",
    "def in_dictionary(token, dictionary):\n",
    "  punctuations = '''![]{};'\"\\,<>./?@#$%^&*~'''\n",
    "\n",
    "  no_punct = token\n",
    "  for char in punctuations:\n",
    "    no_punct = no_punct.replace(char, \" \")\n",
    "  token = no_punct.casefold().strip()\n",
    "\n",
    "  if token in dictionary:\n",
    "    return True\n",
    "  return False\n",
    "\n",
    "\n",
    "def in_entities(token, entities):\n",
    "  punctuations = '''![]{};,'\"\\<>./?@#$%^&*~'''\n",
    "  to_find = token.casefold().strip()\n",
    "\n",
    "  no_punct = to_find\n",
    "  for char in punctuations:\n",
    "    no_punct = no_punct.replace(char, \" \")\n",
    "  to_find = no_punct.casefold().strip()\n",
    "\n",
    "  for label in entities.keys():\n",
    "    if in_dictionary(to_find, entities[label]):\n",
    "      return True, label\n",
    "\n",
    "  return False, \"unknown\"\n",
    "\n",
    "def sliding_window(tokens, entities):\n",
    "  punctuations = '''![]{};,'\"\\<>./?@#$%^&*~'''\n",
    "  classes = []\n",
    "  for token in tokens:\n",
    "    classes.append(\"O\")\n",
    "\n",
    "  for i in range(0, len(tokens)-1):\n",
    "    if (classes[i] is not \"O\"):\n",
    "      next\n",
    "    else:\n",
    "      tmp = tokens[i][0]\n",
    "      if any(x in tmp for x in punctuations):\n",
    "        next\n",
    "\n",
    "      known, label = in_entities(tmp.casefold(), entities)\n",
    "      if known:\n",
    "        classes[i] = \"B-\"+label\n",
    "        tmp = \"\"\n",
    "        next\n",
    "\n",
    "      for j in range(1, 5):\n",
    "        if i+j == len(tokens):\n",
    "          break\n",
    "        if classes[i+j] != \"O\":\n",
    "          break\n",
    "        tmp = tmp + \" \" + tokens[i+j][0]\n",
    "        if any(x in tmp for x in punctuations):\n",
    "          break\n",
    "        known, label = in_entities(tmp.casefold(), entities)\n",
    "        if known:\n",
    "          for x in range(i, i + j + 1):\n",
    "            if x == i:\n",
    "              classes[x] = \"B-\"+label\n",
    "            else:\n",
    "              classes[x] = \"I-\"+label\n",
    "          break\n",
    "\n",
    "  return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ykPJDEltrbeH"
   },
   "outputs": [],
   "source": [
    "# Returns a trained CRF model, and saves to a file\n",
    "\n",
    "def train(file, features, tags, c1, c2, iters, poss_trans, poss_states):\n",
    "  trainer = pycrfsuite.Trainer(verbose=False)\n",
    "\n",
    "  for feature, tag in zip(features, tags):\n",
    "    trainer.append(feature, tag)\n",
    "  \n",
    "  trainer.set_params({\n",
    "    'c1': c1,   # coefficient for L1 penalty\n",
    "    'c2': c2,  # coefficient for L2 penalty\n",
    "    'max_iterations': iters,  # stop earlier\n",
    "\n",
    "    # include transitions that are possible, but not observed\n",
    "    'feature.possible_transitions': poss_trans,\n",
    "    'feature.possible_states': poss_states\n",
    "  })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2t_idAK0FenL"
   },
   "outputs": [],
   "source": [
    "# Returns a CRF sentence tagger, using a given trained model.\n",
    "\n",
    "def getTagger(trainfile):\n",
    "  tagger = pycrfsuite.Tagger()\n",
    "  tagger.open(trainfile)\n",
    "  return tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "okzUi8upLevJ"
   },
   "outputs": [],
   "source": [
    "# Classifies the set of tokens as a set of BIO tags\n",
    "\n",
    "def classify(tokens, known_entities, tagger):\n",
    "  features = extract_features(tokens, known_entities)\n",
    "  return tagger.tag(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iqc4-4RPJomD"
   },
   "outputs": [],
   "source": [
    "# Prints the entities to a file in the required format.\n",
    "# Joins the BIO-tagged tokens into entities.\n",
    "\n",
    "def output_entities(id, tokens, classes, outf):\n",
    "  def join_tags(tokens, classes):\n",
    "    joined_tags = []\n",
    "    joined_tag = \"\"\n",
    "    label = \"\"\n",
    "    span_start = 0\n",
    "    span_end = 0\n",
    "\n",
    "    for i in range(len(tokens)):\n",
    "\n",
    "      if classes[i][:1] == \"B\":\n",
    "        label = classes[i].split(\"-\")[1]\n",
    "        joined_tag= tokens[i][0]\n",
    "        span_start = tokens[i][1][0]\n",
    "        span_end = tokens[i][1][1]\n",
    "      \n",
    "      elif classes[i][:1] == \"I\":\n",
    "        joined_tag = joined_tag + \" \" + tokens[i][0]\n",
    "        span_end = tokens[i][1][1]\n",
    "      \n",
    "      elif (classes[i][:1] == \"O\") and joined_tag.strip():\n",
    "        joined_tags.append([joined_tag, [span_start, span_end], label])\n",
    "        joined_tag = \"\"\n",
    "        span_start = 0\n",
    "        span_end = 0\n",
    "    return joined_tags\n",
    "\n",
    "  joined = join_tags(tokens, classes)\n",
    "  for i in range(len(joined)):\n",
    "    name = joined[i][0]\n",
    "    offset = str(str(joined[i][1][0]) + \"-\" + str(joined[i][1][1]))\n",
    "    label = joined[i][2]\n",
    "    if(label is not \"O\"):\n",
    "      outstring = str(id) + \"|\" + str(offset) + \"|\" + str(name) + \"|\" + str(label) + \"\\n\"\n",
    "      outf.write(outstring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-2ILGgeQMxuY"
   },
   "outputs": [],
   "source": [
    "# Runs the official evaluator on the results\n",
    "\n",
    "def evaluate(inputdir, outputfile):\n",
    "  !java -jar 'drive/My Drive/UPC/Semester2/eval/evaluateNER.jar' \"$inputdir\" \"$outputfile\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YTVcoE6umnhA"
   },
   "source": [
    "# Reading and Writing\n",
    "Functions to read the features file, produced by the AHLT_LAB_1_FEATURE_EXTRACTOR notebook.\n",
    "\n",
    "\n",
    "\n",
    "1.   **features.txt: ** Contains features extracted from the training set.\n",
    "2.   **features_large.txt: ** Contains features extracted from the training set and external sources.\n",
    "3.   **trained.crfsuite: ** Contains trained model extracted from the training set.\n",
    "4.   **trained_large.crfsuite: ** Contains trained model extracted from the training set and external sources.\n",
    "\n",
    "**External data**\n",
    "\n",
    "The external data is in a different form to the training set: it consists of annotated entities with no extraneous words. This leads to biasing of the Position of Speech feature, as entities in the external data are always the only words in the sentence. To combat this, I extracted a random set of non-entity words of random length, and padded each side of the entities. \n",
    "\n",
    "**Sources**\n",
    "\n",
    "The sources of the external data were the DrugBank annotated file, HSDB annotated file, and the 'EN' set of random English sentences, found in the lab directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5sJvj5Vpw5XE"
   },
   "outputs": [],
   "source": [
    "def readFeatures(inputfile):\n",
    "  data = open(inputfile)\n",
    "  features = []\n",
    "  tags = []\n",
    "  sentence_features = []\n",
    "  sentence_tags = []\n",
    "\n",
    "  for line in data.read().splitlines():\n",
    "    if line == \"\\n\" or not line:\n",
    "      features.append(sentence_features)\n",
    "      tags.append(sentence_tags)\n",
    "      sentence_features = []\n",
    "      sentence_tags = []\n",
    "      next\n",
    "    else:\n",
    "      fields = line.split(\"\\t\")\n",
    "      \n",
    "      sentence_id = fields[0]\n",
    "      token = fields[1]\n",
    "      start = fields[2]\n",
    "      end = fields[3]\n",
    "      tag = fields[4]\n",
    "\n",
    "      feature = fields[5:len(fields)]\n",
    "      sentence_features.append(feature)\n",
    "      sentence_tags.append(tag)\n",
    "  return tags, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0liyXcFTFPAd"
   },
   "outputs": [],
   "source": [
    "def read_saved():\n",
    "  prefixes = dict()\n",
    "  prefixes[\"drug\"] = set()\n",
    "  prefixes[\"drug_n\"] = set()\n",
    "  prefixes[\"group\"] = set()\n",
    "  prefixes[\"brand\"] = set()  \n",
    "\n",
    "  suffixes = dict()\n",
    "  suffixes[\"drug\"] = set()\n",
    "  suffixes[\"drug_n\"] = set()\n",
    "  suffixes[\"group\"] = set()\n",
    "  suffixes[\"brand\"] = set()  \n",
    "\n",
    "  entities = dict()\n",
    "  entities[\"drug\"] = set()\n",
    "  entities[\"drug_n\"] = set()\n",
    "  entities[\"group\"] = set()\n",
    "  entities[\"brand\"] = set()\n",
    "\n",
    "  non_entities = set()\n",
    "\n",
    "  prefix_file = open(\"/content/drive/My Drive/UPC/Semester2/data/extracted/prefixes.txt\")\n",
    "  for line in prefix_file.read().splitlines():\n",
    "    split = line.split(\":\")\n",
    "    e_type = split[0]\n",
    "    name = split[1]\n",
    "    prefixes[e_type].add(name.casefold().strip())\n",
    "  prefix_file.close()\n",
    "\n",
    "  suffix_file = open(\"/content/drive/My Drive/UPC/Semester2/data/extracted/suffixes.txt\")\n",
    "  for line in suffix_file.read().splitlines():\n",
    "    split = line.split(\":\")\n",
    "    e_type = split[0]\n",
    "    name = split[1]\n",
    "    suffixes[e_type].add(name.casefold().strip())\n",
    "  suffix_file.close()\n",
    "\n",
    "  entities_file = open(\"/content/drive/My Drive/UPC/Semester2/data/extracted/entities.txt\")\n",
    "  for line in entities_file.read().splitlines():\n",
    "    split = line.split(\":\")\n",
    "    e_type = split[0]\n",
    "    name = split[1]\n",
    "    entities[e_type].add(name.casefold().strip())\n",
    "  entities_file.close()\n",
    "\n",
    "  non_entities_file = open(\"/content/drive/My Drive/UPC/Semester2/data/extracted/non_entities.txt\")\n",
    "  for line in non_entities_file.read().splitlines():\n",
    "    non_entities.add(line.casefold().strip())\n",
    "  non_entities_file.close()\n",
    "\n",
    "  return entities, prefixes, suffixes, non_entities;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FL_foP80FTJK"
   },
   "outputs": [],
   "source": [
    "def read_saved_large():\n",
    "  prefixes = dict()\n",
    "  prefixes[\"drug\"] = set()\n",
    "  prefixes[\"drug_n\"] = set()\n",
    "  prefixes[\"group\"] = set()\n",
    "  prefixes[\"brand\"] = set()  \n",
    "\n",
    "  suffixes = dict()\n",
    "  suffixes[\"drug\"] = set()\n",
    "  suffixes[\"drug_n\"] = set()\n",
    "  suffixes[\"group\"] = set()\n",
    "  suffixes[\"brand\"] = set()  \n",
    "\n",
    "  entities = dict()\n",
    "  entities[\"drug\"] = set()\n",
    "  entities[\"drug_n\"] = set()\n",
    "  entities[\"group\"] = set()\n",
    "  entities[\"brand\"] = set()\n",
    "\n",
    "  non_entities = set()\n",
    "\n",
    "  prefix_file = open(\"/content/drive/My Drive/UPC/Semester2/data/extracted/prefixes_large.txt\")\n",
    "  for line in prefix_file.read().splitlines():\n",
    "    split = line.split(\":\")\n",
    "    e_type = split[0]\n",
    "    name = split[1]\n",
    "    prefixes[e_type].add(name.casefold().strip())\n",
    "  prefix_file.close()\n",
    "\n",
    "  suffix_file = open(\"/content/drive/My Drive/UPC/Semester2/data/extracted/suffixes_large.txt\")\n",
    "  for line in suffix_file.read().splitlines():\n",
    "    split = line.split(\":\")\n",
    "    e_type = split[0]\n",
    "    name = split[1]\n",
    "    suffixes[e_type].add(name.casefold().strip())\n",
    "  suffix_file.close()\n",
    "\n",
    "  entities_file = open(\"/content/drive/My Drive/UPC/Semester2/data/extracted/entities_large.txt\")\n",
    "  for line in entities_file.read().splitlines():\n",
    "    split = line.split(\":\")\n",
    "    e_type = split[0]\n",
    "    name = split[1]\n",
    "    entities[e_type].add(name.casefold().strip())\n",
    "  entities_file.close()\n",
    "\n",
    "  non_entities_file = open(\"/content/drive/My Drive/UPC/Semester2/data/extracted/non_entities_large.txt\")\n",
    "  for line in non_entities_file.read().splitlines():\n",
    "    non_entities.add(line.casefold().strip())\n",
    "  non_entities_file.close()\n",
    "\n",
    "  return entities, prefixes, suffixes, non_entities;"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "AHLT_LAB_1_ML.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
