{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AHLT_LAB_1_DATA_PREP.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLRUpqf3hbDo",
        "colab_type": "text"
      },
      "source": [
        "# Rule Based Drug-Name Classifier - Data Prep\n",
        "This notebook functions to prepare the data for the rule-based classifier for the AHLT course, UPC.\n",
        "\n",
        "Author: Jake Watson, 22/03/2020"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9MsohSnK7Cq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import xml.dom\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "from nltk.tokenize import string_span_tokenize\n",
        "from xml.dom.minidom import parse\n",
        "from nltk.metrics import *\n",
        "from nltk.tokenize.util import align_tokens\n",
        "from google.colab import drive\n",
        "import copy\n",
        "import os\n",
        "\n",
        "\n",
        "def install_java():\n",
        "  !apt-get install -y openjdk-8-jdk-headless -qq > /dev/null      #install openjdk\n",
        "  os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"     #set environment variable\n",
        "  !java -version       #check java version\n",
        "install_java()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqcUhnDJVGbB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81NcteNxjiVw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "entities, suffixes, non_entities = read_large()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XE6y-SBsDJv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "entities, suffixes, non_entities = read_saved_large()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aK4tB-JGTj3E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "entities, prefixes, suffixes, non_entities = extract_training_data('/content/drive/My Drive/UPC/Semester2/data/Train/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYZya577f0rZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "entities, prefixes, suffixes, non_entities = extract_external_data(entities, non_entities)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msAznDGcWLbt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "write_training_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_iICFcwgOzF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "write_large(entities, prefixes, suffixes, non_entities)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtM33Qmqa8Y4",
        "colab_type": "text"
      },
      "source": [
        "# Training Set Analysis\n",
        "This section contains the functions used to extract the known entities and non-entities in the training set.\n",
        "\n",
        "These entities are organized into a dict of sets. The set object was chosen as it is O(1) to check membership of the set, which is useful to check if a potential entity is known. The keys of the dict are the entity classes, allowing us to easily separate the entities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtAypHkiKx0Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extracts the labelled entities, their common suffixes and prefixes, and the non-entities in the training set. All known\n",
        "# entities are organized by their class, using a dict of sets.\n",
        "\n",
        "\n",
        "def extract_training_data(inputdir):\n",
        "\n",
        "  entities = extract_entities(inputdir)\n",
        "  suffixes = extract_suffixes(entities)\n",
        "  prefixes = extract_prefixes(entities)\n",
        "  non_entities = extract_non_entities(inputdir, entities)\n",
        "\n",
        "  return entities, prefixes, suffixes, non_entities"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dGYIOVqOlTi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extracts the set of labelled entities, organized by entity class, in the training set\n",
        "\n",
        "\n",
        "def extract_entities(inputdir):\n",
        "  punctuations = '''![]{};:'\"\\,<>./?@#$%^&*_~'''\n",
        "\n",
        "  entities_by_type = dict()\n",
        "  entities_by_type[\"drug\"] = set()\n",
        "  entities_by_type[\"drug_n\"] = set()\n",
        "  entities_by_type[\"brand\"] = set()\n",
        "  entities_by_type[\"group\"] = set()\n",
        "\n",
        "  for f in os.listdir(inputdir):\n",
        "    tagged = parse(open(str(inputdir) + str(f)))\n",
        "    \n",
        "    entities = tagged.getElementsByTagName(\"entity\")\n",
        "    sentences = tagged.getElementsByTagName(\"sentence\")\n",
        "\n",
        "    for entity in entities:\n",
        "\n",
        "      xml_type = entity.getAttribute('type')\n",
        "      name = entity.getAttribute('text')\n",
        "      \n",
        "      no_punct = name\n",
        "      for char in punctuations:\n",
        "        no_punct = no_punct.replace(char, \" \")\n",
        "      name = no_punct.casefold().strip()\n",
        "\n",
        "      if xml_type == 'drug':\n",
        "        if name not in entities_by_type[\"drug\"]:\n",
        "          entities_by_type[\"drug\"].add(name)\n",
        "      elif xml_type =='drug_n':\n",
        "        if name not in entities_by_type[\"drug_n\"]:\n",
        "          entities_by_type[\"drug_n\"].add(name)\n",
        "      elif xml_type == 'brand':\n",
        "        if name not in entities_by_type[\"brand\"]:\n",
        "          entities_by_type[\"brand\"].add(name)\n",
        "      elif xml_type == 'group':\n",
        "        if name not in entities_by_type[\"group\"]:\n",
        "          entities_by_type[\"group\"].add(name)\n",
        " \n",
        "  return entities_by_type"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PFaZEGbtO0wk",
        "colab": {}
      },
      "source": [
        "# Extracts the set of common entity prefixes, organized by entity class, in the training set\n",
        "\n",
        "\n",
        "def extract_prefixes(entities):\n",
        "  prefixes = dict()\n",
        "  prefixes[\"drug\"] = set()\n",
        "  prefixes[\"drug_n\"] = set()\n",
        "  prefixes[\"brand\"] = set()\n",
        "  prefixes[\"group\"] = set()\n",
        "\n",
        "  for e_type in entities.keys():\n",
        "    raw_prefixes = []\n",
        "    for ent in entities[e_type]:\n",
        "      raw_prefixes.append(ent[:5])\n",
        "    frequencies = {s:raw_prefixes.count(s) for s in raw_prefixes}\n",
        "    pres = set()\n",
        "    for (key, val) in frequencies.items():\n",
        "      if (val > 10 or val/len(entities[e_type]) > 0.001):\n",
        "        pres.add(key.casefold().strip())\n",
        "    prefixes[e_type] = pres \n",
        " \n",
        "  return prefixes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ul7KgEl6OGdV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extracts the set of common entity suffixes, organized by entity class, in the training set\n",
        "\n",
        "def extract_suffixes(entities):\n",
        "  suffixes_by_type = dict()\n",
        "  suffixes_by_type[\"drug\"] = set()\n",
        "  suffixes_by_type[\"drug_n\"] = set()\n",
        "  suffixes_by_type[\"brand\"] = set()\n",
        "  suffixes_by_type[\"group\"] = set()\n",
        "\n",
        "  for e_type in entities.keys():\n",
        "    raw_suffixes = []\n",
        "    for ent in entities[e_type]:\n",
        "      raw_suffixes.append(ent[-5:])\n",
        "\n",
        "    frequencies = {s:raw_suffixes.count(s) for s in raw_suffixes}\n",
        "    suffixes = set()\n",
        "    for (key, val) in frequencies.items():\n",
        "      if (val > 10 or val/len(entities[e_type]) > 0.001):\n",
        "        suffixes.add(key.casefold().strip())\n",
        "\n",
        "    suffixes_by_type[e_type] = suffixes \n",
        " \n",
        "  return suffixes_by_type"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpTMgvZPOegS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extracts all non-labelled words in the training set\n",
        "\n",
        "def extract_non_entities(inputdir, entities):\n",
        "  punctuations = '''![]{};:'\"\\,<>./?@#$%^&*_~'''\n",
        "  \n",
        "  combined = set()  \n",
        "  for e_type in entities.keys():\n",
        "    combined = combined.union(entities[e_type])  \n",
        "\n",
        "  splitstrings = copy.deepcopy(combined)\n",
        "  for item in combined:\n",
        "    tokens = TreebankWordTokenizer().tokenize(item)\n",
        "    for token in tokens:\n",
        "      splitstrings.add(token.casefold().strip())\n",
        "\n",
        "  non_entities = set()\n",
        "\n",
        "  for f in os.listdir(inputdir):\n",
        "    tagged = parse(open(str(inputdir) + str(f)))\n",
        "    sentences = tagged.getElementsByTagName(\"sentence\")\n",
        "\n",
        "    for sentence in sentences:\n",
        "      stext = sentence.attributes[\"text\"].value\n",
        "\n",
        "      no_punct = stext\n",
        "      for char in punctuations:\n",
        "        no_punct = no_punct.replace(char, \" \")\n",
        "      stext = no_punct.casefold().strip()\n",
        "\n",
        "      tokens = TreebankWordTokenizer().tokenize(stext)\n",
        "      for token in tokens:\n",
        "        if token.casefold().strip() not in splitstrings:\n",
        "          non_entities.add(token)\n",
        " \n",
        "  return non_entities"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9K5aS9fxcr_G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Modifies the set of known entities and non-entities with data from external knowledge sources.\n",
        "\n",
        "def extract_external_data(entities, non_entities):\n",
        "  punctuations = '''![]{};:'\"\\,<>./?@#$%^&*_~'''\n",
        "\n",
        "  drugbank = open('drive/My Drive/UPC/Semester2/external/DrugBank.txt')\n",
        "  en = open('drive/My Drive/UPC/Semester2/external/en.txt')\n",
        "  hsdb = open('drive/My Drive/UPC/Semester2/external/HSDB.txt')\n",
        "\n",
        "  for line in drugbank.read().splitlines():\n",
        "    split = line.split(\"|\")\n",
        "    e_type = split[1]\n",
        "    name = split[0].casefold().strip()\n",
        "    no_punct = name\n",
        "    for char in punctuations:\n",
        "      no_punct = no_punct.replace(char, \" \")\n",
        "    name = no_punct\n",
        "    entities[e_type].add(name)\n",
        "  drugbank.close()\n",
        "\n",
        "  counter = 0\n",
        "  for line in hsdb.read().splitlines():\n",
        "    name = line.casefold().strip()\n",
        "    no_punct = name\n",
        "    for char in punctuations:\n",
        "      no_punct = no_punct.replace(char, \" \")\n",
        "    name = no_punct\n",
        "    if name not in entities[\"drug\"] and name not in entities[\"brand\"]  and name not in entities[\"group\"]:\n",
        "      entities[\"drug_n\"].add(name)\n",
        "      counter += 1\n",
        "  hsdb.close()\n",
        "\n",
        "  for e_type in entities.keys():\n",
        "    for entity in entities[e_type]:\n",
        "      if entity in non_entities:\n",
        "        non_entities.remove(entity)\n",
        "\n",
        "  punctuations = '''![]{};:'\"\\,<>./?@#$%^&*_~'''\n",
        "  combined = set()  \n",
        "  for e_type in entities.keys():\n",
        "    combined = combined.union(entities[e_type])  \n",
        "  splitstrings = copy.deepcopy(combined)\n",
        "\n",
        "  for line in en.read().splitlines():\n",
        "    no_punct = line.casefold().strip()\n",
        "    for char in punctuations:\n",
        "      no_punct = no_punct.replace(char, \" \")\n",
        "    text = TreebankWordTokenizer().tokenize(no_punct)\n",
        "\n",
        "    for token in text:\n",
        "      if token.casefold().strip() not in splitstrings:\n",
        "        non_entities.add(token.casefold().strip())\n",
        "\n",
        "  prefixes = extract_prefixes(entities)\n",
        "  suffixes = extract_suffixes(entities)\n",
        "\n",
        "  return entities, prefixes, suffixes, non_entities"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_F_vRrdY5l0",
        "colab_type": "text"
      },
      "source": [
        "# Data Analysis\n",
        "These functions are utilities to perform basic data analysis, or to check for data consistency."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8M2oPwyFrjyM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to check training file consistency.\n",
        "# Checks the dictionary of entities for repeated values in multple sets,\n",
        "# and checks there is no formatting.\n",
        "\n",
        "def is_data_correct():\n",
        "  punctuations = '''![]{};'\"\\,<>./?@#$%^&*~'''\n",
        "\n",
        "  ents = open('/content/drive/My Drive/UPC/Semester2/data/extracted/entities_large.txt')\n",
        "\n",
        "  for line in ents.read().splitlines():\n",
        "    if line.isupper(): \n",
        "      print(\"Line is upper\")\n",
        "      print(line)\n",
        "      return False \n",
        "    if any(x.isupper() for x in line):\n",
        "      print(\"Line has upper\")\n",
        "      print(line)\n",
        "      return False\n",
        "    \n",
        "    for char in line:\n",
        "      if char in punctuations: \n",
        "        print(\"Line has punc\")\n",
        "        print(line)\n",
        "        return False\n",
        "  \n",
        "  entities, suffixes, non_entities = read_saved_large()\n",
        "\n",
        "  count = 0\n",
        "  offenders = set()\n",
        "  for e_type in entities.keys():\n",
        "    for entity in entities[e_type]:\n",
        "      if entity in non_entities:\n",
        "        count += 1\n",
        "        offenders.add(entity)\n",
        "\n",
        "  if count > 0:\n",
        "    print(\"Entity also in nonentities\")\n",
        "    print(len(offenders))\n",
        "    print(offenders)\n",
        "    return False\n",
        "\n",
        "  return True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRaKogmPqaJW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Checks how many entities in a list end with 's'\n",
        "\n",
        "def ends_with_s(entities):\n",
        "  count = 0\n",
        "  for entity in entities:\n",
        "    if entity[-1:].casefold().strip() == \"s\":\n",
        "      count += 1\n",
        "  return count/len(entities)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xe-lFVit_Cz5",
        "colab": {}
      },
      "source": [
        "# Checks how many entities in a list have one capitalised character\n",
        "\n",
        "def one_capital(entity):\n",
        "  count = 0 \n",
        "  for entity in entities:\n",
        "      if entity.isupper() == False and any(x.isupper() for x in entity):\n",
        "        count += 1\n",
        "  return count/len(entity)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJg2NjsCTHpE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Returns the minimum and maximum lengths of strings in a list\n",
        "\n",
        "def min_max_lengths(entities):\n",
        "  for e_type in entities.keys():\n",
        "    minimum = 1000\n",
        "    maximum = 0\n",
        "    minvalue = \"\"\n",
        "    maxvalue = \"\"\n",
        "    for name in entities[e_type]:\n",
        "      length = len(name)\n",
        "      if (length < minimum):\n",
        "        minimum = length\n",
        "        minvalue = name\n",
        "      elif (length > maximum):\n",
        "        maximum = length\n",
        "        maxvalue = name\n",
        "    print(e_type)\n",
        "    print(minvalue)\n",
        "    print(minimum)\n",
        "    print(maxvalue)\n",
        "    print(maximum)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sk8KTt3B17Jy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Finds the set of entity classes in the training data\n",
        "\n",
        "def find_entity_types():\n",
        "  types = []\n",
        "  for fil in os.listdir('drive/My Drive/UPC/Semester2/data/Train/'):\n",
        "    f = str('drive/My Drive/UPC/Semester2/data/Train/') + str(fil) \n",
        "    fil = open(f)\n",
        "    tagged = parseXML(fil)\n",
        "    entities = tagged.getElementsByTagName(\"entity\")\n",
        "\n",
        "    for entity in entities:\\\n",
        "      xml_type = entity.getAttribute('type')\n",
        "      types.append(xml_type)\n",
        "  \n",
        "  types = list(dict.fromkeys(types) )\n",
        "  return types"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AglKgJZgYgwc",
        "colab_type": "text"
      },
      "source": [
        "# Reading and writing training files\n",
        "These functions are for reading or saving training data to files, for later use.\n",
        "\n",
        "1.   **entities.txt, prefixes.txt, suffixes.txt, non_entities.txt** Contains known entities extracted from the training set.\n",
        "2.   **entities_large.txt, prefixes_large.txt, suffixes_large.txt, non_entities_large.txt** Contains known entities extracted from the training set and external sources.\n",
        "\n",
        "**Sources**\n",
        "\n",
        "The sources of the external data were the DrugBank annotated file, HSDB annotated file, and the 'EN' set of random English sentences, found in the lab directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPIRupGYvvsT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def write_training_data():\n",
        "  inputdir = 'drive/My Drive/UPC/Semester2/data/Train/'\n",
        "  entities, prefixes, suffixes, non_entities = extract_training_data(inputdir)\n",
        "\n",
        "  with open('/content/drive/My Drive/UPC/Semester2/data/extracted/entities.txt', 'w+') as entities_file:\n",
        "    for e_type in entities.keys():\n",
        "      for name in entities[e_type]:\n",
        "        entities_file.write(e_type + \":\" + name + \"\\n\") \n",
        "  !cat /content/drive/My\\ Drive/UPC/Semester2/data/extracted/entities.txt\n",
        "\n",
        "  with open('/content/drive/My Drive/UPC/Semester2/data/extracted/prefixes.txt', 'w+') as prefixes_file:\n",
        "    for e_type in prefixes.keys():\n",
        "      for name in prefixes[e_type]:\n",
        "        prefixes_file.write(e_type + \":\" + name + \"\\n\") \n",
        "  !cat /content/drive/My\\ Drive/UPC/Semester2/data/extracted/prefixes.txt\n",
        "  \n",
        "  with open('/content/drive/My Drive/UPC/Semester2/data/extracted/suffixes.txt', 'w+') as suffixes_file:\n",
        "    for e_type in suffixes.keys():\n",
        "      for name in suffixes[e_type]:\n",
        "        suffixes_file.write(e_type + \":\" + name + \"\\n\") \n",
        "  !cat /content/drive/My\\ Drive/UPC/Semester2/data/extracted/suffixes.txt\n",
        "\n",
        "  with open('/content/drive/My Drive/UPC/Semester2/data/extracted/non_entities.txt', 'w+') as non_entities_file:\n",
        "    for name in non_entities:\n",
        "      non_entities_file.write(name + \"\\n\") \n",
        "  !cat /content/drive/My\\ Drive/UPC/Semester2/data/extracted/non_entities.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kykuUdi-jSbD",
        "colab": {}
      },
      "source": [
        "def write_large(entities, prefixes, suffixes, non_entities):\n",
        "  with open('/content/drive/My Drive/UPC/Semester2/data/extracted/entities_large.txt', 'w+') as entities_file:\n",
        "    for e_type in entities.keys():\n",
        "      for name in entities[e_type]:\n",
        "        entities_file.write(e_type + \":\" + name + \"\\n\") \n",
        "  !cat /content/drive/My\\ Drive/UPC/Semester2/data/extracted/entities_large.txt\n",
        "\n",
        "  with open('/content/drive/My Drive/UPC/Semester2/data/extracted/prefixes_large.txt', 'w+') as prefixes_file:\n",
        "    for e_type in prefixes.keys():\n",
        "      for name in prefixes[e_type]:\n",
        "        prefixes_file.write(e_type + \":\" + name + \"\\n\") \n",
        "  !cat /content/drive/My\\ Drive/UPC/Semester2/data/extracted/prefixes.txt\n",
        "  \n",
        "  with open('/content/drive/My Drive/UPC/Semester2/data/extracted/suffixes_large.txt', 'w+') as suffixes_file:\n",
        "    for e_type in suffixes.keys():\n",
        "      for name in suffixes[e_type]:\n",
        "        suffixes_file.write(e_type + \":\" + name + \"\\n\") \n",
        "  !cat /content/drive/My\\ Drive/UPC/Semester2/data/extracted/suffixes_large.txt\n",
        "\n",
        "  with open('/content/drive/My Drive/UPC/Semester2/data/extracted/non_entities_large.txt', 'w+') as non_entities_file:\n",
        "    for name in non_entities:\n",
        "      non_entities_file.write(name + \"\\n\") \n",
        "  !cat /content/drive/My\\ Drive/UPC/Semester2/data/extracted/non_entities_large.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykPJDEltrbeH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_saved():\n",
        "  prefixes = dict()\n",
        "  prefixes[\"drug\"] = set()\n",
        "  prefixes[\"drug_n\"] = set()\n",
        "  prefixes[\"brand\"] = set()\n",
        "  prefixes[\"group\"] = set()\n",
        "  \n",
        "  suffixes = dict()\n",
        "  suffixes[\"drug\"] = set()\n",
        "  suffixes[\"drug_n\"] = set()\n",
        "  suffixes[\"group\"] = set()\n",
        "  suffixes[\"brand\"] = set()  \n",
        "\n",
        "  entities = dict()\n",
        "  entities[\"drug\"] = set()\n",
        "  entities[\"drug_n\"] = set()\n",
        "  entities[\"group\"] = set()\n",
        "  entities[\"brand\"] = set()\n",
        "\n",
        "  non_entities = set()\n",
        "\n",
        "  prefix_file = open(\"/content/drive/My Drive/UPC/Semester2/data/extracted/prefixes.txt\")\n",
        "  for line in prefix_file.read().splitlines():\n",
        "    split = line.split(\":\")\n",
        "    e_type = split[0]\n",
        "    name = split[1]\n",
        "    prefixes[e_type].add(name.casefold().strip())\n",
        "  prefix_file.close()\n",
        "\n",
        "  suffix_file = open(\"/content/drive/My Drive/UPC/Semester2/data/extracted/suffixes.txt\")\n",
        "  for line in suffix_file.read().splitlines():\n",
        "    split = line.split(\":\")\n",
        "    e_type = split[0]\n",
        "    name = split[1]\n",
        "    suffixes[e_type].add(name.casefold().strip())\n",
        "  suffix_file.close()\n",
        "\n",
        "  entities_file = open(\"/content/drive/My Drive/UPC/Semester2/data/extracted/entities.txt\")\n",
        "  for line in entities_file.read().splitlines():\n",
        "    split = line.split(\":\")\n",
        "    e_type = split[0]\n",
        "    name = split[1]\n",
        "    entities[e_type].add(name.casefold().strip())\n",
        "  entities_file.close()\n",
        "\n",
        "  non_entities_file = open(\"/content/drive/My Drive/UPC/Semester2/data/extracted/non_entities.txt\")\n",
        "  for line in non_entities_file.read().splitlines():\n",
        "    non_entities.add(line.casefold().strip())\n",
        "  non_entities_file.close()\n",
        "\n",
        "  return entities, prefixes, suffixes, non_entities;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5sJvj5Vpw5XE",
        "colab": {}
      },
      "source": [
        "def read_saved_large():\n",
        "  prefixes = dict()\n",
        "  prefixes[\"drug\"] = set()\n",
        "  prefixes[\"drug_n\"] = set()\n",
        "  prefixes[\"group\"] = set()\n",
        "  prefixes[\"brand\"] = set()  \n",
        "\n",
        "  suffixes = dict()\n",
        "  suffixes[\"drug\"] = set()\n",
        "  suffixes[\"drug_n\"] = set()\n",
        "  suffixes[\"group\"] = set()\n",
        "  suffixes[\"brand\"] = set()  \n",
        "\n",
        "  entities = dict()\n",
        "  entities[\"drug\"] = set()\n",
        "  entities[\"drug_n\"] = set()\n",
        "  entities[\"group\"] = set()\n",
        "  entities[\"brand\"] = set()\n",
        "\n",
        "  non_entities = set()\n",
        "\n",
        "  prefix_file = open(\"/content/drive/My Drive/UPC/Semester2/data/extracted/prefixes_large.txt\")\n",
        "  for line in prefix_file.read().splitlines():\n",
        "    split = line.split(\":\")\n",
        "    e_type = split[0]\n",
        "    name = split[1]\n",
        "    prefixes[e_type].add(name.casefold().strip())\n",
        "  prefix_file.close()\n",
        "\n",
        "  suffix_file = open(\"/content/drive/My Drive/UPC/Semester2/data/extracted/suffixes_large.txt\")\n",
        "  for line in suffix_file.read().splitlines():\n",
        "    split = line.split(\":\")\n",
        "    e_type = split[0]\n",
        "    name = split[1]\n",
        "    suffixes[e_type].add(name.casefold().strip())\n",
        "  suffix_file.close()\n",
        "\n",
        "  entities_file = open(\"/content/drive/My Drive/UPC/Semester2/data/extracted/entities_large.txt\")\n",
        "  for line in entities_file.read().splitlines():\n",
        "    split = line.split(\":\")\n",
        "    e_type = split[0]\n",
        "    name = split[1]\n",
        "    entities[e_type].add(name.casefold().strip())\n",
        "  entities_file.close()\n",
        "\n",
        "  non_entities_file = open(\"/content/drive/My Drive/UPC/Semester2/data/extracted/non_entities_large.txt\")\n",
        "  for line in non_entities_file.read().splitlines():\n",
        "    non_entities.add(line.casefold().strip())\n",
        "  non_entities_file.close()\n",
        "\n",
        "  return entities, prefixes, suffixes, non_entities;"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}