{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AHLT_LAB_1_RB.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ttwJOc1bmLO",
        "colab_type": "text"
      },
      "source": [
        "# Rule Based Drug-Name Classifier\n",
        "This notebook contains the rule-based classifier for the AHLT course, UPC.\n",
        "\n",
        "Author: Jake Watson, 22/03/2020\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9MsohSnK7Cq",
        "colab_type": "code",
        "outputId": "e984fa67-b8f9-4de4-debd-4511008872f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        }
      },
      "source": [
        "import xml.dom\n",
        "from xml.dom.minidom import parse\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "from nltk.tokenize import string_span_tokenize\n",
        "from nltk.metrics import *\n",
        "from nltk.tokenize.util import align_tokens\n",
        "from google.colab import drive\n",
        "import copy\n",
        "import os\n",
        "import sys\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "def install_java():\n",
        "  !apt-get install -y openjdk-8-jdk-headless -qq > /dev/null      #install openjdk\n",
        "  os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"     #set environment variable\n",
        "  !java -version       #check java version\n",
        "install_java()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "openjdk version \"11.0.6\" 2020-01-14\n",
            "OpenJDK Runtime Environment (build 11.0.6+10-post-Ubuntu-1ubuntu118.04.1)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.6+10-post-Ubuntu-1ubuntu118.04.1, mixed mode, sharing)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nn4-MzMYb_Pi",
        "colab_type": "text"
      },
      "source": [
        "Loads the data taken from the training set.\n",
        "```\n",
        "read_saved()\n",
        "```\n",
        "Loads the data taken from the training set and external knowledge sources\n",
        "```\n",
        "read_saved_large()\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8M-swGg0HAL2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "entities, prefixes, suffixes, non_entities = read_saved()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBKkd5_ZlHGG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "entities, prefixes, suffixes, non_entities = read_saved_large()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxZb2q5bdQlL",
        "colab_type": "text"
      },
      "source": [
        "The following two fields show instances of the evaluator output for the NERC function, for the 'Devel' and 'Test-NER' datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "2fddb211-f8be-4c6a-e386-5cfc5970463d",
        "id": "vfsPGdtCclHG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "nerc('drive/My Drive/UPC/Semester2/data/Devel/','task9.1_RBexternalDevel_1.txt', entities, prefixes, suffixes, non_entities, 0.9)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Considering file 85/85: Gemcitabine_ddi.xmlGold drive/My Drive/UPC/Semester2/data/Devel/\n",
            "Submission  task9.1_RBexternalDevel_1.txt\n",
            "Directory gold drive/My Drive/UPC/Semester2/data/Devel/\n",
            "[transDirXMLToMapEntities] dir:drive/My Drive/UPC/Semester2/data/Devel/\n",
            "log4j:WARN No appenders could be found for logger (org.castor.core.util.Configuration).\n",
            "log4j:WARN Please initialize the log4j system properly.\n",
            " Gold standard saved in goldNER.txt\n",
            "\n",
            "Gold loaded. Sentences=681, entities: 1771\n",
            "task9.1_RBexternalDevel_1_scores.log created...\n",
            "SCORES FOR THE GROUP: RBexternalDevel RUN=1\n",
            "\n",
            "Warning!!!! some sentences are no included in the gold!!!\n",
            "\n",
            "Strict matching (boundaries + type)\n",
            "cor\tinc\tpar\tmis\tspu\ttotal\tprec\trecall\tF1\n",
            "1327\t268\t0\t176\t517\t1771\t0.63\t0.75\t0.68\n",
            "\n",
            "\n",
            "\n",
            "Warning!!!! some sentences are no included in the gold!!!\n",
            "\n",
            "Exact matching\n",
            "cor\tinc\tpar\tmis\tspu\ttotal\tprec\trecall\tF1\n",
            "1474\t121\t0\t176\t517\t1771\t0.7\t0.83\t0.76\n",
            "\n",
            "\n",
            "\n",
            "Warning!!!! some sentences are no included in the gold!!!\n",
            "\n",
            "Partial matching\n",
            "cor\tinc\tpar\tmis\tspu\ttotal\tprec\trecall\tF1\n",
            "1474\t0\t121\t176\t517\t1771\t0.7\t0.87\t0.77\n",
            "\n",
            "\n",
            "\n",
            "Warning!!!! some sentences are no included in the gold!!!\n",
            "\n",
            "type matching\n",
            "cor\tinc\tpar\tmis\tspu\ttotal\tprec\trecall\tF1\n",
            "1398\t197\t0\t176\t517\t1771\t0.66\t0.79\t0.72\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "SCORES FOR ENTITY TYPE\n",
            "Warning!!!! some sentences are no included in the gold!!!\n",
            "\n",
            "Exact matching on drug\n",
            "cor\tinc\tpar\tmis\tspu\ttotal\tprec\trecall\tF1\n",
            "939\t6\t0\t100\t273\t1045\t0.77\t0.9\t0.83\n",
            "\n",
            "\n",
            "Warning!!!! some sentences are no included in the gold!!!\n",
            "\n",
            "Exact matching on brand\n",
            "cor\tinc\tpar\tmis\tspu\ttotal\tprec\trecall\tF1\n",
            "160\t1\t0\t19\t12\t180\t0.92\t0.89\t0.91\n",
            "\n",
            "\n",
            "Warning!!!! some sentences are no included in the gold!!!\n",
            "\n",
            "Exact matching on group\n",
            "cor\tinc\tpar\tmis\tspu\ttotal\tprec\trecall\tF1\n",
            "214\t65\t0\t175\t83\t454\t0.59\t0.47\t0.52\n",
            "\n",
            "\n",
            "Warning!!!! some sentences are no included in the gold!!!\n",
            "\n",
            "Exact matching on drug_n\n",
            "cor\tinc\tpar\tmis\tspu\ttotal\tprec\trecall\tF1\n",
            "14\t0\t0\t78\t1\t92\t0.93\t0.15\t0.26\n",
            "\n",
            "\n",
            "MACRO-AVERAGE MEASURES:\n",
            "P\tR\tF1\n",
            "0.81\t0.6\t0.63\n",
            "________________________________________________________________________\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiKShRkC6oBE",
        "colab_type": "code",
        "outputId": "05b965e9-08dc-4aa5-9995-3f35494b0b7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "nerc('drive/My Drive/UPC/Semester2/data/Test-NER/','task9.1_RBexternalTest_1.txt', entities, prefixes, suffixes, non_entities, 0.9)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Considering file 112/112: Terconazole.xmlGold drive/My Drive/UPC/Semester2/data/Test-NER/\n",
            "Submission  task9.1_RBexternalTest_1.txt\n",
            "Directory gold drive/My Drive/UPC/Semester2/data/Test-NER/\n",
            "[transDirXMLToMapEntities] dir:drive/My Drive/UPC/Semester2/data/Test-NER/\n",
            "log4j:WARN No appenders could be found for logger (org.castor.core.util.Configuration).\n",
            "log4j:WARN Please initialize the log4j system properly.\n",
            " Gold standard saved in goldNER.txt\n",
            "\n",
            "Gold loaded. Sentences=324, entities: 686\n",
            "task9.1_RBexternalTest_1_scores.log created...\n",
            "SCORES FOR THE GROUP: RBexternalTest RUN=1\n",
            "\n",
            "Warning!!!! some sentences are no included in the gold!!!\n",
            "\n",
            "Strict matching (boundaries + type)\n",
            "cor\tinc\tpar\tmis\tspu\ttotal\tprec\trecall\tF1\n",
            "420\t144\t0\t122\t305\t686\t0.48\t0.61\t0.54\n",
            "\n",
            "\n",
            "\n",
            "Warning!!!! some sentences are no included in the gold!!!\n",
            "\n",
            "Exact matching\n",
            "cor\tinc\tpar\tmis\tspu\ttotal\tprec\trecall\tF1\n",
            "518\t46\t0\t122\t305\t686\t0.6\t0.76\t0.67\n",
            "\n",
            "\n",
            "\n",
            "Warning!!!! some sentences are no included in the gold!!!\n",
            "\n",
            "Partial matching\n",
            "cor\tinc\tpar\tmis\tspu\ttotal\tprec\trecall\tF1\n",
            "518\t0\t46\t122\t305\t686\t0.6\t0.79\t0.68\n",
            "\n",
            "\n",
            "\n",
            "Warning!!!! some sentences are no included in the gold!!!\n",
            "\n",
            "type matching\n",
            "cor\tinc\tpar\tmis\tspu\ttotal\tprec\trecall\tF1\n",
            "443\t121\t0\t122\t305\t686\t0.51\t0.65\t0.57\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "SCORES FOR ENTITY TYPE\n",
            "Warning!!!! some sentences are no included in the gold!!!\n",
            "\n",
            "Exact matching on drug\n",
            "cor\tinc\tpar\tmis\tspu\ttotal\tprec\trecall\tF1\n",
            "299\t9\t0\t43\t111\t351\t0.71\t0.85\t0.78\n",
            "\n",
            "\n",
            "Warning!!!! some sentences are no included in the gold!!!\n",
            "\n",
            "Exact matching on brand\n",
            "cor\tinc\tpar\tmis\tspu\ttotal\tprec\trecall\tF1\n",
            "43\t3\t0\t13\t5\t59\t0.84\t0.73\t0.78\n",
            "\n",
            "\n",
            "Warning!!!! some sentences are no included in the gold!!!\n",
            "\n",
            "Exact matching on group\n",
            "cor\tinc\tpar\tmis\tspu\ttotal\tprec\trecall\tF1\n",
            "76\t12\t0\t67\t43\t155\t0.58\t0.49\t0.53\n",
            "\n",
            "\n",
            "Warning!!!! some sentences are no included in the gold!!!\n",
            "\n",
            "Exact matching on drug_n\n",
            "cor\tinc\tpar\tmis\tspu\ttotal\tprec\trecall\tF1\n",
            "2\t0\t0\t119\t1\t121\t0.67\t0.02\t0.03\n",
            "\n",
            "\n",
            "MACRO-AVERAGE MEASURES:\n",
            "P\tR\tF1\n",
            "0.7\t0.52\t0.53\n",
            "________________________________________________________________________\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9l2XC4hiPUJh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Main function: parses the XML files, extracts the sentences, tokenizes them, labels each token, outputs the results, and evaluate the results.\n",
        "\n",
        "def nerc(inputdir, outputfile, ents, pres, sufs, nes, sensitivity):\n",
        "  output = open(outputfile, \"w+\")\n",
        "  count = 1\n",
        "  n_files = len(os.listdir(inputdir))\n",
        "  for fil in os.listdir(inputdir):\n",
        "    sys.stdout.write(\"\\rConsidering file \" + str(count) + \"/\" + str(n_files) + \": \" + str(fil))\n",
        "    sys.stdout.flush()\n",
        "    count += 1\n",
        "    fil = open(str(inputdir) + str(fil))\n",
        "    tree = parse(fil)\n",
        "    fil.close()\n",
        "    sentences = tree.getElementsByTagName(\"sentence\")\n",
        "    for sentence in sentences:\n",
        "      sid = sentence.attributes[\"id\"].value\n",
        "      stext = sentence.attributes[\"text\"].value\n",
        "      tokens = tokenize(stext)\n",
        "      labelled = extract_entities(tokens, ents, pres, sufs, nes, sensitivity)\n",
        "      output_entities(sid, labelled, output)\n",
        "  output.close\n",
        "  evaluate(inputdir, outputfile)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWCEDJF3jtkC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenizes the input text. \n",
        "# Returns the tokens, with their offsets from the beginning of the sentence.\n",
        "\n",
        "def tokenize(input):\n",
        "  s = input.replace('\"', \"'\")\n",
        "  tokens = TreebankWordTokenizer().tokenize(s)\n",
        "  offsets = list(align_tokens(tokens, s))\n",
        "  offsets = [tuple((i, j-1)) for i, j in offsets]\n",
        "  output = [tuple((i, j)) for i, j in zip(tokens, offsets)]\n",
        "  return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bpPlp3MQWwf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Classifies the set of tokens.\n",
        "# Requires the set of known entities, their common suffixes and prefixes, and the non-entities.\n",
        "\n",
        "def extract_entities(tokens, entities, prefixes, suffixes, non_entities, sensitivity):\n",
        "  labelled = applyRules(tokens, entities, prefixes, suffixes, non_entities, sensitivity)\n",
        "  string_output = []\n",
        "  for (token, offsets, typ) in labelled:\n",
        "    string_output.append(\n",
        "        {\n",
        "          \"name\": str(token),\n",
        "          \"offset\": str(str(offsets[0]) + \"-\" + str(offsets[1])),\n",
        "          \"type\": str(typ)\n",
        "        })\n",
        "  return string_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuoD1vT_Ys0m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prints the entities to a file in the required format.\n",
        "\n",
        "def output_entities(id, ents, outf):\n",
        "  for entity in ents:\n",
        "    name = entity[\"name\"]\n",
        "    offset = entity[\"offset\"]\n",
        "    typ = entity[\"type\"]\n",
        "    outstring = str(id) + \"|\" + str(offset) + \"|\" + str(name) + \"|\" + str(typ) + \"\\n\"\n",
        "    outf.write(outstring)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2ILGgeQMxuY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Runs the official evaluator on the results\n",
        "\n",
        "def evaluate(inputdir, outputfile):\n",
        "  !java -jar 'drive/My Drive/UPC/Semester2/eval/evaluateNER.jar' \"$inputdir\" \"$outputfile\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgCfecHzev9a",
        "colab_type": "text"
      },
      "source": [
        "# Rules\n",
        "The following functions attempt to classify a given set of tokens. This is done by applying a set of rules to the token. If the token satisfies a rule for a certain class, it is given a score for that class. After applying all rules for all classes, these scores are summed for each class, and the token is classified according to the maximum scoring class.\n",
        "\n",
        "The scores for each class are normalized to be between 0 and 1, and a threshold is applied: if no score is above the threshold, the token is not classified.\n",
        "\n",
        "The rules applied are as follows:\n",
        "\n",
        "\n",
        "*   Capitalised: is the token entirely capitalised?\n",
        "*   Has Capitals: does the token contain any capitals?\n",
        "*   Dashes: does the token contain dashes (-) ?\n",
        "*   Numbers: does the token contain numbers?\n",
        "*   Ends with S: does the token end with 's'?\n",
        "*   Has suffix/prefix: does the token have a common prefix or suffix?\n",
        "\n",
        "\n",
        "Finally, the token is also checked against a list of known entities. If it is known, then no other rules are checked and the token is classified according to the list. \n",
        "\n",
        "To ensure multi-word entities are also found by this method, the sliding-window method is applied. A sliding window of length 5 tokens is moved across the sentence, with each set of tokens in the window being checked against the list of known entities. Additionally, once classified, if there ny adjacent tokens with the same classification, they are joined together as one entity by the 'adjacent_join' function.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBG2IKKLnN2j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Returns the list of classified entities in a sentence.\n",
        "# Applies the set of rules to a sentence, given the set of known entities and non-entities.\n",
        "\n",
        "def applyRules(tokens, entities, prefixes, suffixes, non_entities, sensitivity):\n",
        "\n",
        "  labelled_drugs = []\n",
        "  labelled_drugs_n = []\n",
        "  labelled_groups = []\n",
        "  labelled_brands = []\n",
        "\n",
        "\n",
        "  #Functions to return the score for a token in each class.\n",
        "\n",
        "  def drug_score(token, drugs, pres, suffs):\n",
        "    score = 0\n",
        "    score += is_capitalised(token)\n",
        "    score += has_dashes(token)\n",
        "    score += has_numbers(token)\n",
        "    score += has_suffix(token, suffs)\n",
        "    score += has_prefix(token, pres)\n",
        "    return score / 5.0\n",
        "  \n",
        "  def group_score(token, groups, pres, suffs):\n",
        "    score = 0\n",
        "    score += 1 - is_capitalised(token)\n",
        "    score += ends_with_s(token)\n",
        "    score += has_suffix(token, suffs)\n",
        "    score += has_prefix(token, pres)\n",
        "    return score / 4.0\n",
        "\n",
        "  def brand_score(token, brands, pres, suffs):\n",
        "    score = 0\n",
        "    score += is_capitalised(token)\n",
        "    score += has_capitals(token)\n",
        "    score += has_suffix(token, suffs)\n",
        "    score += has_prefix(token, pres)\n",
        "    return score / 4.0\n",
        "\n",
        "  # Check for and remove known multi-word entities\n",
        "  remaining, labelled = sliding_window(tokens, entities)\n",
        "\n",
        "  for (token, offset, label) in labelled:\n",
        "    if label == \"drug\":\n",
        "      labelled_drugs.append((token, offset, \"drug\"))\n",
        "    elif label == \"drug_n\":\n",
        "      labelled_drugs_n.append((token, offset, \"drug_n\"))\n",
        "    elif label == \"group\":\n",
        "      labelled_groups.append((token, offset, \"group\"))\n",
        "    elif label  == \"brand\":\n",
        "      labelled_brands.append((token, offset, \"brand\"))\n",
        "\n",
        "\n",
        "\n",
        "  for (token, offset) in remaining:\n",
        "\n",
        "    # Check if known nonentity\n",
        "    if in_dictionary(token, non_entities) is True:\n",
        "      next\n",
        "\n",
        "    # Check if known entity\n",
        "    known, e_type = in_entities(token, entities)\n",
        "\n",
        "    if known:\n",
        "      if e_type == \"drug\":\n",
        "        labelled_drugs.append((token, offset, \"drug\"))\n",
        "      elif e_type == \"drug_n\":\n",
        "        labelled_drugs_n.append((token, offset, \"drug_n\"))\n",
        "      elif e_type == \"group\":\n",
        "        labelled_groups.append((token, offset, \"group\"))\n",
        "      elif e_type == \"brand\":\n",
        "        labelled_brands.append((token, offset, \"brand\"))\n",
        "      next\n",
        "\n",
        "    # If not known entity or non-entity, check scores for each class\n",
        "    else:\n",
        "      d_score = drug_score(token, entities[\"drug\"], prefixes[\"drug\"], suffixes[\"drug\"])\n",
        "      dn_score = drug_score(token, entities[\"drug_n\"], prefixes[\"drug_n\"], suffixes[\"drug_n\"])\n",
        "      g_score = group_score(token, entities[\"group\"], prefixes[\"group\"], suffixes[\"group\"])\n",
        "      b_score = brand_score(token, entities[\"brand\"], prefixes[\"brand\"], suffixes[\"brand\"])\n",
        "\n",
        "    # If the maximum scoring class is above the threshold, classify the token\n",
        "      if max(d_score, dn_score, g_score, b_score) >= sensitivity:\n",
        "        maxScore = max(d_score, dn_score, g_score, b_score)\n",
        "        if maxScore == d_score:\n",
        "          labelled_drugs.append((token, offset, \"drug\"))\n",
        "        elif maxScore == dn_score:\n",
        "          labelled_drugs_n.append((token, offset, \"drug_n\"))\n",
        "        elif maxScore == g_score:\n",
        "          labelled_groups.append((token, offset, \"group\"))\n",
        "        elif maxScore == b_score:\n",
        "          labelled_brands.append((token, offset, \"brand\"))\n",
        "\n",
        "  # Two passes of adjacent join - joins up to three adjacent words    \n",
        "  joineddrugs1 = adjacentjoin(labelled_drugs)\n",
        "  joineddrugs2 = adjacentjoin(joineddrugs1)\n",
        "\n",
        "  joineddrugs_n_1 = adjacentjoin(labelled_drugs_n)\n",
        "  joineddrugs_n_2 = adjacentjoin(joineddrugs_n_1)\n",
        "\n",
        "  joinedgroups1 = adjacentjoin(labelled_groups)\n",
        "  joinedgroups2 = adjacentjoin(joinedgroups1)\n",
        "\n",
        "  joinedbrands1 = adjacentjoin(labelled_brands)\n",
        "  joinedbrands2 = adjacentjoin(joinedbrands1)\n",
        "\n",
        "  # Mix the data together again and sort by the sentence position - putting the tokens in order\n",
        "  mixed = sorted(joineddrugs2 + joineddrugs_n_2 + joinedgroups2 + joinedbrands2, key = lambda x: x[1][0])\n",
        "  \n",
        "  return mixed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "569xFpKcpa2a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sliding_window(tokens, entities):\n",
        "  punctuations = '''![]{};,'\"\\<>./?@#$%^&*~'''\n",
        "  no_punct = []\n",
        "  joined = []\n",
        "  cut_tokens = copy.deepcopy(tokens)\n",
        "\n",
        "  for i in range(0, len(tokens)-1):\n",
        "    tmp = tokens[i][0]\n",
        "    start_offset = tokens[i][1][0]\n",
        "    if any(x in tmp for x in punctuations):\n",
        "      next\n",
        "    for j in range(1, 5):\n",
        "      if i+j == len(tokens):\n",
        "        break\n",
        "      end_offset = tokens[i+j][1][1]\n",
        "      tmp = tmp + \" \" + tokens[i+j][0]\n",
        "      if any(x in tmp for x in punctuations):\n",
        "        break\n",
        "      known, typ = in_entities(tmp.casefold(), entities)\n",
        "      if known:\n",
        "        for x in range(i, i + j + 1):\n",
        "          cut_tokens[x] = None\n",
        "        combined = [tmp, [start_offset, end_offset], typ]\n",
        "        joined.append(combined)\n",
        "        break\n",
        "\n",
        "\n",
        "  final_tokens = []\n",
        "  for elem in cut_tokens:\n",
        "    if elem is not None:\n",
        "      final_tokens.append(elem)\n",
        "  \n",
        "  return final_tokens, joined\n",
        "\n",
        "def is_capitalised(token):\n",
        "  if token.isupper():\n",
        "    return 1\n",
        "  return 0\n",
        "\n",
        "def has_capitals(token):\n",
        "  if (token.isupper() == False) and any(x.isupper() for x in token):\n",
        "    return 1\n",
        "  else: \n",
        "    return 0\n",
        "\n",
        "def has_dashes(token):\n",
        "  if (\"-\" in token):\n",
        "    return 1\n",
        "  return 0\n",
        "\n",
        "def has_numbers(token):\n",
        "  if any(i.isdigit() for i in token):\n",
        "    return 1\n",
        "  return 0\n",
        "\n",
        "def ends_with_s(token):\n",
        "  if token[-1:].casefold().strip() == 's':\n",
        "    return 1\n",
        "  else: \n",
        "    return 0\n",
        "\n",
        "def has_suffix(token, suffs):\n",
        "  for suffix in suffs:\n",
        "    if (suffix == token.casefold()[-len(suffix.strip()):]):\n",
        "      return 1\n",
        "  return 0\n",
        "\n",
        "def has_prefix(token, prefs):\n",
        "  for prefix in prefs:\n",
        "    if (prefix == token.casefold()[:len(prefix.strip())]):\n",
        "      return 1\n",
        "  return 0\n",
        "\n",
        "def in_dictionary(token, dictionary):\n",
        "  punctuations = '''![]{};'\"\\,<>./?@#$%^&*~'''\n",
        "\n",
        "  no_punct = token\n",
        "  for char in punctuations:\n",
        "    no_punct = no_punct.replace(char, \" \")\n",
        "  token = no_punct.casefold().strip()\n",
        "\n",
        "  if token in dictionary:\n",
        "    return True\n",
        "  return False\n",
        "\n",
        "def in_entities(token, entities):\n",
        "\n",
        "  for typ in entities.keys():\n",
        "    if in_dictionary(token, entities[typ]):\n",
        "      return True, typ\n",
        "\n",
        "  return False, \"unknown\"\n",
        "\n",
        "def adjacentjoin(tokens):\n",
        "  def listit(t):\n",
        "    return list(map(listit, t)) if isinstance(t, (list, tuple)) else t\n",
        "  unjoined = listit(tokens)\n",
        "\n",
        "  for i in range(1, len(unjoined)):\n",
        "    prev = unjoined[i-1]\n",
        "    curr = unjoined[i]\n",
        "\n",
        "    prev_end = prev[1][1]\n",
        "    curr_start = curr[1][0]\n",
        "\n",
        "    if ((prev_end + 2) == curr_start):\n",
        "      unjoined[i-1][0] = (str(prev[0]) + \" \" + str(curr[0]))\n",
        "      unjoined[i-1][1][1] = curr[1][1]\n",
        "\n",
        "      unjoined[i][0] = \"\"\n",
        "      unjoined[i][1] = [0,0]\n",
        "\n",
        "  joined = []\n",
        "  for i in range (len(unjoined)):\n",
        "    if unjoined[i][0] != \"\":\n",
        "      joined.append(unjoined[i])\n",
        "  return joined"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhEH3mxNhJpK",
        "colab_type": "text"
      },
      "source": [
        "# Reading and Writing\n",
        "Functions to read the training data produced by the AHLT_DATA_PREP notebook.\n",
        "\n",
        "1.   **entities.txt, prefixes.txt, suffixes.txt, non_entities.txt** Contains known entities extracted from the training set.\n",
        "2.   **entities_large.txt, prefixes_large.txt, suffixes_large.txt, non_entities_large.txt** Contains known entities extracted from the training set and external sources.\n",
        "\n",
        "**Sources**\n",
        "\n",
        "The sources of the external data were the DrugBank annotated file, HSDB annotated file, and the 'EN' set of random English sentences, found in the lab directory.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykPJDEltrbeH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_saved():\n",
        "  prefixes = dict()\n",
        "  prefixes[\"drug\"] = set()\n",
        "  prefixes[\"drug_n\"] = set()\n",
        "  prefixes[\"group\"] = set()\n",
        "  prefixes[\"brand\"] = set()  \n",
        "\n",
        "  suffixes = dict()\n",
        "  suffixes[\"drug\"] = set()\n",
        "  suffixes[\"drug_n\"] = set()\n",
        "  suffixes[\"group\"] = set()\n",
        "  suffixes[\"brand\"] = set()  \n",
        "\n",
        "  entities = dict()\n",
        "  entities[\"drug\"] = set()\n",
        "  entities[\"drug_n\"] = set()\n",
        "  entities[\"group\"] = set()\n",
        "  entities[\"brand\"] = set()\n",
        "\n",
        "  non_entities = set()\n",
        "\n",
        "  prefix_file = open(\"/content/drive/My Drive/UPC/Semester2/data/extracted/prefixes.txt\")\n",
        "  for line in prefix_file.read().splitlines():\n",
        "    split = line.split(\":\")\n",
        "    e_type = split[0]\n",
        "    name = split[1]\n",
        "    prefixes[e_type].add(name.casefold().strip())\n",
        "  prefix_file.close()\n",
        "\n",
        "  suffix_file = open(\"/content/drive/My Drive/UPC/Semester2/data/extracted/suffixes.txt\")\n",
        "  for line in suffix_file.read().splitlines():\n",
        "    split = line.split(\":\")\n",
        "    e_type = split[0]\n",
        "    name = split[1]\n",
        "    suffixes[e_type].add(name.casefold().strip())\n",
        "  suffix_file.close()\n",
        "\n",
        "  entities_file = open(\"/content/drive/My Drive/UPC/Semester2/data/extracted/entities.txt\")\n",
        "  for line in entities_file.read().splitlines():\n",
        "    split = line.split(\":\")\n",
        "    e_type = split[0]\n",
        "    name = split[1]\n",
        "    entities[e_type].add(name.casefold().strip())\n",
        "  entities_file.close()\n",
        "\n",
        "  non_entities_file = open(\"/content/drive/My Drive/UPC/Semester2/data/extracted/non_entities.txt\")\n",
        "  for line in non_entities_file.read().splitlines():\n",
        "    non_entities.add(line.casefold().strip())\n",
        "  non_entities_file.close()\n",
        "\n",
        "  return entities, prefixes, suffixes, non_entities;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5sJvj5Vpw5XE",
        "colab": {}
      },
      "source": [
        "def read_saved_large():\n",
        "  prefixes = dict()\n",
        "  prefixes[\"drug\"] = set()\n",
        "  prefixes[\"drug_n\"] = set()\n",
        "  prefixes[\"group\"] = set()\n",
        "  prefixes[\"brand\"] = set()  \n",
        "\n",
        "  suffixes = dict()\n",
        "  suffixes[\"drug\"] = set()\n",
        "  suffixes[\"drug_n\"] = set()\n",
        "  suffixes[\"group\"] = set()\n",
        "  suffixes[\"brand\"] = set()  \n",
        "\n",
        "  entities = dict()\n",
        "  entities[\"drug\"] = set()\n",
        "  entities[\"drug_n\"] = set()\n",
        "  entities[\"group\"] = set()\n",
        "  entities[\"brand\"] = set()\n",
        "\n",
        "  non_entities = set()\n",
        "\n",
        "  prefix_file = open(\"/content/drive/My Drive/UPC/Semester2/data/extracted/prefixes_large.txt\")\n",
        "  for line in prefix_file.read().splitlines():\n",
        "    split = line.split(\":\")\n",
        "    e_type = split[0]\n",
        "    name = split[1]\n",
        "    prefixes[e_type].add(name.casefold().strip())\n",
        "  prefix_file.close()\n",
        "\n",
        "  suffix_file = open(\"/content/drive/My Drive/UPC/Semester2/data/extracted/suffixes_large.txt\")\n",
        "  for line in suffix_file.read().splitlines():\n",
        "    split = line.split(\":\")\n",
        "    e_type = split[0]\n",
        "    name = split[1]\n",
        "    suffixes[e_type].add(name.casefold().strip())\n",
        "  suffix_file.close()\n",
        "\n",
        "  entities_file = open(\"/content/drive/My Drive/UPC/Semester2/data/extracted/entities_large.txt\")\n",
        "  for line in entities_file.read().splitlines():\n",
        "    split = line.split(\":\")\n",
        "    e_type = split[0]\n",
        "    name = split[1]\n",
        "    entities[e_type].add(name.casefold().strip())\n",
        "  entities_file.close()\n",
        "\n",
        "  non_entities_file = open(\"/content/drive/My Drive/UPC/Semester2/data/extracted/non_entities_large.txt\")\n",
        "  for line in non_entities_file.read().splitlines():\n",
        "    non_entities.add(line.casefold().strip())\n",
        "  non_entities_file.close()\n",
        "\n",
        "  return entities, prefixes, suffixes, non_entities;"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}